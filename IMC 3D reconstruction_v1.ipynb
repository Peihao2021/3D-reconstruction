{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe22984c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T09:22:57.800175Z",
     "iopub.status.busy": "2024-06-11T09:22:57.799836Z",
     "iopub.status.idle": "2024-06-11T09:22:57.804809Z",
     "shell.execute_reply": "2024-06-11T09:22:57.804063Z"
    },
    "papermill": {
     "duration": 0.013134,
     "end_time": "2024-06-11T09:22:57.806740",
     "exception": false,
     "start_time": "2024-06-11T09:22:57.793606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#模型与数据集链接\n",
    "#https://www.kaggle.com/competitions/image-matching-challenge-2024\n",
    "#https://www.kaggle.com/datasets/maxchen303/hardnet8v2\n",
    "#https://www.kaggle.com/datasets/oldufo/imc2024-packages-lightglue-rerun-kornia\n",
    "#https://www.kaggle.com/datasets/oldufo/kornia-local-feature-weights\n",
    "#https://www.kaggle.com/datasets/losveria/super-glue-pretrained-network\n",
    "#https://www.kaggle.com/models/oldufo/aliked/PyTorch/aliked-n16/1\n",
    "#https://www.kaggle.com/models/timm/tf-efficientnet/PyTorch/tf-efficientnet-b7/1\n",
    "#https://www.kaggle.com/models/timm/tf-efficientnet/PyTorch/tf-efficientnet-b6/1\n",
    "#https://www.kaggle.com/code/motono0223/pytorch-lightglue-models\n",
    "#https://www.kaggle.com/code/motono0223/dkm-dependencies\n",
    "#https://www.kaggle.com/code/motono0223/dependencies-imc\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaa4d267",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-11T09:22:57.817133Z",
     "iopub.status.busy": "2024-06-11T09:22:57.816500Z",
     "iopub.status.idle": "2024-06-11T09:24:17.749553Z",
     "shell.execute_reply": "2024-06-11T09:24:17.748535Z"
    },
    "papermill": {
     "duration": 79.940678,
     "end_time": "2024-06-11T09:24:17.752057",
     "exception": false,
     "start_time": "2024-06-11T09:22:57.811379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./kaggle/input/dependencies-imc/pycolmap/pycolmap-0.4.0-cp39-cp39-manylinux2014_x86_64.whl\n",
      "Installing collected packages: pycolmap\n",
      "  Attempting uninstall: pycolmap\n",
      "    Found existing installation: pycolmap 3.10.0\n",
      "    Uninstalling pycolmap-3.10.0:\n",
      "      Successfully uninstalled pycolmap-3.10.0\n",
      "Successfully installed pycolmap-0.4.0\n",
      "Processing ./kaggle/input/dependencies-imc/safetensors/safetensors-0.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Installing collected packages: safetensors\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.4.4\n",
      "    Uninstalling safetensors-0.4.4:\n",
      "      Successfully uninstalled safetensors-0.4.4\n",
      "Successfully installed safetensors-0.4.1\n",
      "Processing ./kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\n",
      "lightglue is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --no-deps ~/user/3D-reconstruction/kaggle/input/dependencies-imc/pycolmap/pycolmap-0.4.0-cp39-cp39-manylinux2014_x86_64.whl\n",
    "!python -m pip install --no-deps ~/user/3D-reconstruction/kaggle/input/dependencies-imc/safetensors/safetensors-0.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!python -m pip install --no-index --find-links=~/user/3D-reconstruction/kaggle/input/dependencies-imc/transformers/ transformers > /dev/null\n",
    "!python -m pip install  --no-deps ~/user/3D-reconstruction/kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f9c1edc",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-11T09:24:17.774623Z",
     "iopub.status.busy": "2024-06-11T09:24:17.774308Z",
     "iopub.status.idle": "2024-06-11T09:24:17.779986Z",
     "shell.execute_reply": "2024-06-11T09:24:17.779304Z"
    },
    "papermill": {
     "duration": 0.013912,
     "end_time": "2024-06-11T09:24:17.781868",
     "exception": false,
     "start_time": "2024-06-11T09:24:17.767956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b858357",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-11T09:24:17.793360Z",
     "iopub.status.busy": "2024-06-11T09:24:17.792915Z",
     "iopub.status.idle": "2024-06-11T09:24:27.394365Z",
     "shell.execute_reply": "2024-06-11T09:24:27.393589Z"
    },
    "papermill": {
     "duration": 9.609665,
     "end_time": "2024-06-11T09:24:27.396606",
     "exception": false,
     "start_time": "2024-06-11T09:24:17.786941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LoFTR' from 'kornia.feature' (/home/zhr/miniconda3/envs/rec/lib/python3.9/site-packages/kornia/feature/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 44\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# LoFTR\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkornia\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoFTR\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LoFTR' from 'kornia.feature' (/home/zhr/miniconda3/envs/rec/lib/python3.9/site-packages/kornia/feature/__init__.py)"
     ]
    }
   ],
   "source": [
    "# General utilities\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from fastprogress import progress_bar\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import concurrent.futures\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "import collections\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# CV/ML\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "from PIL import Image\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "import torchvision\n",
    "\n",
    "# 3D reconstruction\n",
    "import pycolmap\n",
    "\n",
    "import glob\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# LoFTR\n",
    "from kornia.feature import LoFTR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc942b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zhr/miniconda3/envs/rec\n",
      "Package                   Version\n",
      "------------------------- --------------\n",
      "absl-py                   2.1.0\n",
      "aiohappyeyeballs          2.4.3\n",
      "aiohttp                   3.10.10\n",
      "aiosignal                 1.3.1\n",
      "albumentations            0.5.1\n",
      "altgraph                  0.17.2\n",
      "anyio                     4.6.2.post1\n",
      "appnope                   0.1.4\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "astroid                   3.3.5\n",
      "asttokens                 2.4.1\n",
      "async-lru                 2.0.4\n",
      "async-timeout             4.0.3\n",
      "attrs                     24.2.0\n",
      "autopep8                  2.3.1\n",
      "babel                     2.16.0\n",
      "beautifulsoup4            4.12.3\n",
      "bleach                    6.1.0\n",
      "certifi                   2024.6.2\n",
      "cffi                      1.17.1\n",
      "charset-normalizer        3.3.2\n",
      "click                     8.1.7\n",
      "comm                      0.2.2\n",
      "contourpy                 1.2.1\n",
      "cycler                    0.12.1\n",
      "debugpy                   1.8.1\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.7.1\n",
      "dill                      0.3.9\n",
      "einops                    0.3.0\n",
      "exceptiongroup            1.2.1\n",
      "executing                 2.0.1\n",
      "fastjsonschema            2.20.0\n",
      "fastprogress              1.0.3\n",
      "filelock                  3.14.0\n",
      "fonttools                 4.53.0\n",
      "fqdn                      1.5.1\n",
      "frozenlist                1.5.0\n",
      "fsspec                    2024.6.0\n",
      "future                    0.18.2\n",
      "google                    3.0.0\n",
      "grpcio                    1.67.1\n",
      "h11                       0.14.0\n",
      "h5py                      3.11.0\n",
      "httpcore                  1.0.6\n",
      "httpx                     0.27.2\n",
      "huggingface-hub           0.24.6\n",
      "idna                      3.7\n",
      "imageio                   2.36.0\n",
      "imgaug                    0.4.0\n",
      "importlib_metadata        7.1.0\n",
      "importlib_resources       6.4.0\n",
      "ipykernel                 6.29.4\n",
      "ipython                   8.18.1\n",
      "isoduration               20.11.0\n",
      "isort                     5.13.2\n",
      "jedi                      0.19.1\n",
      "Jinja2                    3.1.4\n",
      "joblib                    1.4.2\n",
      "json5                     0.9.28\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.23.0\n",
      "jsonschema-specifications 2024.10.1\n",
      "jupyter_client            8.6.2\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.10.0\n",
      "jupyter-lsp               2.2.5\n",
      "jupyter_server            2.14.2\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.3.0\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.27.3\n",
      "kaggle                    1.6.17\n",
      "kagglehub                 0.2.9\n",
      "kiwisolver                1.4.5\n",
      "kornia                    0.7.3\n",
      "kornia_rs                 0.1.5\n",
      "lazy_loader               0.4\n",
      "lightglue                 0.0\n",
      "loguru                    0.5.3\n",
      "macholib                  1.15.2\n",
      "Markdown                  3.7\n",
      "MarkupSafe                2.1.5\n",
      "matplotlib                3.9.0\n",
      "matplotlib-inline         0.1.7\n",
      "mccabe                    0.7.0\n",
      "mistune                   3.0.2\n",
      "mpmath                    1.3.0\n",
      "msgpack                   1.1.0\n",
      "multidict                 6.1.0\n",
      "nbclient                  0.10.0\n",
      "nbconvert                 7.16.4\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "networkx                  3.2.1\n",
      "notebook_shim             0.2.4\n",
      "numpy                     1.26.4\n",
      "nvidia-cublas-cu12        12.1.3.1\n",
      "nvidia-cuda-cupti-cu12    12.1.105\n",
      "nvidia-cuda-nvrtc-cu12    12.1.105\n",
      "nvidia-cuda-runtime-cu12  12.1.105\n",
      "nvidia-cudnn-cu12         8.9.2.26\n",
      "nvidia-cufft-cu12         11.0.2.54\n",
      "nvidia-curand-cu12        10.3.2.106\n",
      "nvidia-cusolver-cu12      11.4.5.107\n",
      "nvidia-cusparse-cu12      12.1.0.106\n",
      "nvidia-nccl-cu12          2.20.5\n",
      "nvidia-nvjitlink-cu12     12.6.20\n",
      "nvidia-nvtx-cu12          12.1.105\n",
      "opencv-python             4.10.0.82\n",
      "overrides                 7.7.0\n",
      "packaging                 24.0\n",
      "pandas                    2.2.2\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "pexpect                   4.9.0\n",
      "pillow                    10.3.0\n",
      "pip                       24.2\n",
      "platformdirs              4.2.2\n",
      "prometheus_client         0.21.0\n",
      "prompt_toolkit            3.0.46\n",
      "propcache                 0.2.0\n",
      "protobuf                  5.28.3\n",
      "psutil                    5.9.8\n",
      "ptyprocess                0.7.0\n",
      "pure-eval                 0.2.2\n",
      "py-cpuinfo                9.0.0\n",
      "pycodestyle               2.12.1\n",
      "pycolmap                  3.10.0\n",
      "pycparser                 2.22\n",
      "pyDeprecate               0.3.0\n",
      "Pygments                  2.18.0\n",
      "pylint                    3.3.1\n",
      "pyparsing                 3.1.2\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        2.0.7\n",
      "python-slugify            8.0.4\n",
      "pytorch-lightning         1.3.5\n",
      "pytz                      2024.1\n",
      "PyYAML                    6.0.1\n",
      "pyzmq                     26.0.3\n",
      "ray                       2.39.0\n",
      "referencing               0.35.1\n",
      "regex                     2024.7.24\n",
      "requests                  2.32.3\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rpds-py                   0.21.0\n",
      "safetensors               0.4.4\n",
      "scikit-image              0.24.0\n",
      "scikit-learn              1.5.1\n",
      "scipy                     1.13.1\n",
      "seaborn                   0.13.2\n",
      "Send2Trash                1.8.3\n",
      "setuptools                58.0.4\n",
      "shapely                   2.0.6\n",
      "six                       1.15.0\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.5\n",
      "stack-data                0.6.3\n",
      "sympy                     1.12.1\n",
      "tensorboard               2.18.0\n",
      "tensorboard-data-server   0.7.2\n",
      "terminado                 0.18.1\n",
      "text-unidecode            1.3\n",
      "threadpoolctl             3.5.0\n",
      "tifffile                  2024.8.30\n",
      "timm                      1.0.9\n",
      "tinycss2                  1.4.0\n",
      "tokenizers                0.19.1\n",
      "tomli                     2.1.0\n",
      "tomlkit                   0.13.2\n",
      "torch                     2.3.0\n",
      "torchmetrics              0.6.0\n",
      "torchvision               0.18.0\n",
      "tornado                   6.4\n",
      "tqdm                      4.66.4\n",
      "traitlets                 5.14.3\n",
      "transformers              4.40.1\n",
      "triton                    2.3.0\n",
      "types-python-dateutil     2.9.0.20241003\n",
      "typing_extensions         4.12.1\n",
      "tzdata                    2024.1\n",
      "ultralytics               8.2.28\n",
      "ultralytics-thop          0.2.7\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.2.1\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 24.11.1\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "Werkzeug                  3.1.3\n",
      "wheel                     0.37.0\n",
      "yacs                      0.1.8\n",
      "yarl                      1.17.1\n",
      "zipp                      3.19.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.prefix)\n",
    "%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23fe6951",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T09:24:27.408886Z",
     "iopub.status.busy": "2024-06-11T09:24:27.408581Z",
     "iopub.status.idle": "2024-06-11T09:24:27.413556Z",
     "shell.execute_reply": "2024-06-11T09:24:27.412526Z"
    },
    "papermill": {
     "duration": 0.013149,
     "end_time": "2024-06-11T09:24:27.415536",
     "exception": false,
     "start_time": "2024-06-11T09:24:27.402387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kornia version 0.7.3\n",
      "Pycolmap version 0.4.0\n"
     ]
    }
   ],
   "source": [
    "print('Kornia version', K.__version__)\n",
    "print('Pycolmap version', pycolmap.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "416dd0dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T09:24:27.428545Z",
     "iopub.status.busy": "2024-06-11T09:24:27.428302Z",
     "iopub.status.idle": "2024-06-11T09:37:13.205630Z",
     "shell.execute_reply": "2024-06-11T09:37:13.204563Z"
    },
    "papermill": {
     "duration": 765.787066,
     "end_time": "2024-06-11T09:37:13.207965",
     "exception": false,
     "start_time": "2024-06-11T09:24:27.420899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "church / church -> 41 images\n",
      "church\n",
      "church\n",
      "Got 41 images\n",
      "rotation_detection for 41 images : 0.0000 sec\n",
      "41!!!!!!!!!!!\n",
      "GO!\n",
      "820, pairs to match, 0.0007 sec\n",
      "local feature: GFTT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14417/3432634763.py:956: DeprecationWarning: `LAFAffNetShapeEstimator` default behaviour is changed and now it does preserve original LAF orientation. Make sure your code accounts for this.\n",
      "  aff_module=KF.LAFAffNetShapeEstimator(False).eval(),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='41' class='' max='41' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [41/41 00:27&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 768, 1024]) to torch.Size([1, 3, 600, 800]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 768, 1024]) to torch.Size([1, 3, 600, 800]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 768, 1024]) to torch.Size([1, 3, 600, 800]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 768, 1024]) to torch.Size([1, 3, 600, 800]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 768, 1024]) to torch.Size([1, 3, 600, 800]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "Resized torch.Size([1, 3, 1024, 768]) to torch.Size([1, 3, 800, 600]) (resize_small_edge_to=600)\n",
      "keypoints_for local feature model:222099\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='820' class='' max='820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [820/820 01:45&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"http://cmp.felk.cvut.cz/~mishkdmy/models/loftr_outdoor.ckpt\" to /home/zhr/.cache/torch/hub/checkpoints/loftr_outdoor.ckpt\n",
      "100%|██████████| 44.2M/44.2M [00:12<00:00, 3.64MB/s]\n",
      "  0%|          | 0/820 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 576.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1996\u001b[0m\n\u001b[1;32m   1991\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m   1993\u001b[0m \u001b[38;5;66;03m#############################################################\u001b[39;00m\n\u001b[1;32m   1994\u001b[0m \u001b[38;5;66;03m# get keypoints\u001b[39;00m\n\u001b[1;32m   1995\u001b[0m \u001b[38;5;66;03m#############################################################            \u001b[39;00m\n\u001b[0;32m-> 1996\u001b[0m keypoints_timings \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper_keypoints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg_fnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrots\u001b[49m\n\u001b[1;32m   1998\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1999\u001b[0m timings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_matching\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m keypoints_timings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_matching\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m   2000\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "Cell \u001b[0;32mIn[13], line 1444\u001b[0m, in \u001b[0;36mwrapper_keypoints\u001b[0;34m(img_fnames, index_pairs, feature_dir, device, timings, rots)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     file_keypoints \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/matches_loftr.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1441\u001b[0m     params_loftr \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1442\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_matches\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m15\u001b[39m,\n\u001b[1;32m   1443\u001b[0m     }\n\u001b[0;32m-> 1444\u001b[0m     \u001b[43mdetect_loftr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_fnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_loftr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_keypoints\u001b[49m\n\u001b[1;32m   1446\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1447\u001b[0m     files_keypoints\u001b[38;5;241m.\u001b[39mappend(file_keypoints)\n\u001b[1;32m   1448\u001b[0m \u001b[38;5;66;03m# if CONFIG.use_sift:\u001b[39;00m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;66;03m#     file_keypoints = f\"{feature_dir}/matches_sift.h5\"\u001b[39;00m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;66;03m#     params_sift = {\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;66;03m# merge keypoints\u001b[39;00m\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;66;03m#############################################################\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 1333\u001b[0m, in \u001b[0;36mdetect_loftr\u001b[0;34m(img_fnames, index_pairs, feature_dir, device, params, file_keypoints)\u001b[0m\n\u001b[1;32m   1327\u001b[0m batch \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1328\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage0\u001b[39m\u001b[38;5;124m'\u001b[39m: image_tensors[img1_path],\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage1\u001b[39m\u001b[38;5;124m'\u001b[39m: image_tensors[img2_path]\n\u001b[1;32m   1330\u001b[0m }\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m-> 1333\u001b[0m     \u001b[43mmatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1335\u001b[0m mkpts0 \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m   1336\u001b[0m mkpts1 \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/miniconda3/envs/rec/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rec/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rec/lib/python3.9/site-packages/kornia/feature/loftr/loftr.py:164\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/rec/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rec/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rec/lib/python3.9/site-packages/kornia/feature/loftr/utils/coarse_matching.py:124\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, feat_c0, feat_c1, data, mask_c0, mask_c1)\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 576.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "class CONFIG:\n",
    "    # DEBUG Settings\n",
    "    DRY_RUN = False\n",
    "    DRY_RUN_MAX_IMAGES = 10\n",
    "\n",
    "    # Pipeline settings\n",
    "    NUM_CORES = 2\n",
    "    \n",
    "    # COLMAP Reconstruction\n",
    "    CAMERA_MODEL = \"simple-radial\"\n",
    "    \n",
    "    # Rotation correction\n",
    "    ROTATION_CORRECTION = True\n",
    "    \n",
    "    # Keypoints handling\n",
    "    MERGE_PARAMS = {\n",
    "        \"min_matches\" : 15,\n",
    "        \"filter_FundamentalMatrix\" : False,\n",
    "        \"filter_iterations\" : 10,\n",
    "        \"filter_threshold\" : 8,\n",
    "    }\n",
    "    \n",
    "    # Keypoints Extraction\n",
    "    # zhr: modifying here can change the model that is being used. Now we are using superglue\n",
    "    use_aliked_lightglue = 0\n",
    "    use_doghardnet_lightglue = False\n",
    "    use_superpoint_lightglue = False\n",
    "    use_disk_lightglue = False\n",
    "    use_sift_lightglue = False\n",
    "    use_loftr = False\n",
    "    use_dkm = False\n",
    "    use_superglue = 1\n",
    "    use_matchformer = False\n",
    "        \n",
    "    # Keypoints Extraction Parameters\n",
    "    params_aliked_lightglue = {\n",
    "        \"num_features\" : 8000,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 100,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    params_doghardnet_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    params_superpoint_lightglue = {\n",
    "        \"num_features\" : 4096,\n",
    "        \"detection_threshold\" : 0.005,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    \n",
    "    params_sg3 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 50,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1680,\n",
    "        \"min_matches\": 100,\n",
    "    }\n",
    "    params_sgs = [params_sg3,]\n",
    "\n",
    "\n",
    "device=torch.device('cuda')\n",
    "\n",
    "import sys\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "IS_PYTHON3 = sys.version_info[0] >= 3\n",
    "\n",
    "MAX_IMAGE_ID = 2**31 - 1\n",
    "\n",
    "CREATE_CAMERAS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS cameras (\n",
    "    camera_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "    model INTEGER NOT NULL,\n",
    "    width INTEGER NOT NULL,\n",
    "    height INTEGER NOT NULL,\n",
    "    params BLOB,\n",
    "    prior_focal_length INTEGER NOT NULL)\"\"\"\n",
    "\n",
    "CREATE_DESCRIPTORS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS descriptors (\n",
    "    image_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\"\"\"\n",
    "\n",
    "CREATE_IMAGES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS images (\n",
    "    image_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "    name TEXT NOT NULL UNIQUE,\n",
    "    camera_id INTEGER NOT NULL,\n",
    "    prior_qw REAL,\n",
    "    prior_qx REAL,\n",
    "    prior_qy REAL,\n",
    "    prior_qz REAL,\n",
    "    prior_tx REAL,\n",
    "    prior_ty REAL,\n",
    "    prior_tz REAL,\n",
    "    CONSTRAINT image_id_check CHECK(image_id >= 0 and image_id < {}),\n",
    "    FOREIGN KEY(camera_id) REFERENCES cameras(camera_id))\n",
    "\"\"\".format(MAX_IMAGE_ID)\n",
    "\n",
    "CREATE_TWO_VIEW_GEOMETRIES_TABLE = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS two_view_geometries (\n",
    "    pair_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    config INTEGER NOT NULL,\n",
    "    F BLOB,\n",
    "    E BLOB,\n",
    "    H BLOB)\n",
    "\"\"\"\n",
    "\n",
    "CREATE_KEYPOINTS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS keypoints (\n",
    "    image_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\n",
    "\"\"\"\n",
    "\n",
    "CREATE_MATCHES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS matches (\n",
    "    pair_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB)\"\"\"\n",
    "\n",
    "CREATE_NAME_INDEX = \\\n",
    "    \"CREATE UNIQUE INDEX IF NOT EXISTS index_name ON images(name)\"\n",
    "\n",
    "CREATE_ALL = \"; \".join([\n",
    "    CREATE_CAMERAS_TABLE,\n",
    "    CREATE_IMAGES_TABLE,\n",
    "    CREATE_KEYPOINTS_TABLE,\n",
    "    CREATE_DESCRIPTORS_TABLE,\n",
    "    CREATE_MATCHES_TABLE,\n",
    "    CREATE_TWO_VIEW_GEOMETRIES_TABLE,\n",
    "    CREATE_NAME_INDEX\n",
    "])\n",
    "\n",
    "\n",
    "def image_ids_to_pair_id(image_id1, image_id2):\n",
    "    if image_id1 > image_id2:\n",
    "        image_id1, image_id2 = image_id2, image_id1\n",
    "    return image_id1 * MAX_IMAGE_ID + image_id2\n",
    "\n",
    "\n",
    "def pair_id_to_image_ids(pair_id):\n",
    "    image_id2 = pair_id % MAX_IMAGE_ID\n",
    "    image_id1 = (pair_id - image_id2) / MAX_IMAGE_ID\n",
    "    return image_id1, image_id2\n",
    "\n",
    "\n",
    "def array_to_blob(array):\n",
    "    if IS_PYTHON3:\n",
    "        return array.tostring()\n",
    "    else:\n",
    "        return np.getbuffer(array)\n",
    "\n",
    "\n",
    "def blob_to_array(blob, dtype, shape=(-1,)):\n",
    "    if IS_PYTHON3:\n",
    "        return np.fromstring(blob, dtype=dtype).reshape(*shape)\n",
    "    else:\n",
    "        return np.frombuffer(blob, dtype=dtype).reshape(*shape)\n",
    "\n",
    "\n",
    "class COLMAPDatabase(sqlite3.Connection):\n",
    "\n",
    "    @staticmethod\n",
    "    def connect(database_path):\n",
    "        return sqlite3.connect(database_path, factory=COLMAPDatabase)\n",
    "\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(COLMAPDatabase, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.create_tables = lambda: self.executescript(CREATE_ALL)\n",
    "        self.create_cameras_table = \\\n",
    "            lambda: self.executescript(CREATE_CAMERAS_TABLE)\n",
    "        self.create_descriptors_table = \\\n",
    "            lambda: self.executescript(CREATE_DESCRIPTORS_TABLE)\n",
    "        self.create_images_table = \\\n",
    "            lambda: self.executescript(CREATE_IMAGES_TABLE)\n",
    "        self.create_two_view_geometries_table = \\\n",
    "            lambda: self.executescript(CREATE_TWO_VIEW_GEOMETRIES_TABLE)\n",
    "        self.create_keypoints_table = \\\n",
    "            lambda: self.executescript(CREATE_KEYPOINTS_TABLE)\n",
    "        self.create_matches_table = \\\n",
    "            lambda: self.executescript(CREATE_MATCHES_TABLE)\n",
    "        self.create_name_index = lambda: self.executescript(CREATE_NAME_INDEX)\n",
    "\n",
    "    def add_camera(self, model, width, height, params,\n",
    "                   prior_focal_length=False, camera_id=None):\n",
    "        params = np.asarray(params, np.float64)\n",
    "        cursor = self.execute(\n",
    "            \"INSERT INTO cameras VALUES (?, ?, ?, ?, ?, ?)\",\n",
    "            (camera_id, model, width, height, array_to_blob(params),\n",
    "             prior_focal_length))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "    def add_image(self, name, camera_id,\n",
    "                  prior_q=np.zeros(4), prior_t=np.zeros(3), image_id=None):\n",
    "        cursor = self.execute(\n",
    "            \"INSERT INTO images VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "            (image_id, name, camera_id, prior_q[0], prior_q[1], prior_q[2],\n",
    "             prior_q[3], prior_t[0], prior_t[1], prior_t[2]))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "    def add_keypoints(self, image_id, keypoints):\n",
    "        assert(len(keypoints.shape) == 2)\n",
    "        assert(keypoints.shape[1] in [2, 4, 6])\n",
    "\n",
    "        keypoints = np.asarray(keypoints, np.float32)\n",
    "        self.execute(\n",
    "            \"INSERT INTO keypoints VALUES (?, ?, ?, ?)\",\n",
    "            (image_id,) + keypoints.shape + (array_to_blob(keypoints),))\n",
    "\n",
    "    def add_descriptors(self, image_id, descriptors):\n",
    "        descriptors = np.ascontiguousarray(descriptors, np.uint8)\n",
    "        self.execute(\n",
    "            \"INSERT INTO descriptors VALUES (?, ?, ?, ?)\",\n",
    "            (image_id,) + descriptors.shape + (array_to_blob(descriptors),))\n",
    "\n",
    "    def add_matches(self, image_id1, image_id2, matches):\n",
    "        assert(len(matches.shape) == 2)\n",
    "        assert(matches.shape[1] == 2)\n",
    "\n",
    "        if image_id1 > image_id2:\n",
    "            matches = matches[:,::-1]\n",
    "\n",
    "        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n",
    "        matches = np.asarray(matches, np.uint32)\n",
    "        self.execute(\n",
    "            \"INSERT INTO matches VALUES (?, ?, ?, ?)\",\n",
    "            (pair_id,) + matches.shape + (array_to_blob(matches),))\n",
    "\n",
    "    def add_two_view_geometry(self, image_id1, image_id2, matches,\n",
    "                              F=np.eye(3), E=np.eye(3), H=np.eye(3), config=2):\n",
    "        assert(len(matches.shape) == 2)\n",
    "        assert(matches.shape[1] == 2)\n",
    "\n",
    "        if image_id1 > image_id2:\n",
    "            matches = matches[:,::-1]\n",
    "\n",
    "        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n",
    "        matches = np.asarray(matches, np.uint32)\n",
    "        F = np.asarray(F, dtype=np.float64)\n",
    "        E = np.asarray(E, dtype=np.float64)\n",
    "        H = np.asarray(H, dtype=np.float64)\n",
    "        self.execute(\n",
    "            \"INSERT INTO two_view_geometries VALUES (?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "            (pair_id,) + matches.shape + (array_to_blob(matches), config,\n",
    "             array_to_blob(F), array_to_blob(E), array_to_blob(H)))\n",
    "\n",
    "import os, argparse, h5py, warnings\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ExifTags\n",
    "\n",
    "\n",
    "def get_focal(image_path, err_on_default=False):\n",
    "    image         = Image.open(image_path)\n",
    "    max_size      = max(image.size)\n",
    "\n",
    "    exif = image.getexif()\n",
    "    focal = None\n",
    "    if exif is not None:\n",
    "        focal_35mm = None\n",
    "        # https://github.com/colmap/colmap/blob/d3a29e203ab69e91eda938d6e56e1c7339d62a99/src/util/bitmap.cc#L299\n",
    "        for tag, value in exif.items():\n",
    "            focal_35mm = None\n",
    "            if ExifTags.TAGS.get(tag, None) == 'FocalLengthIn35mmFilm':\n",
    "                focal_35mm = float(value)\n",
    "                break\n",
    "\n",
    "        if focal_35mm is not None:\n",
    "            focal = focal_35mm / 35. * max_size\n",
    "    \n",
    "    if focal is None:\n",
    "        if err_on_default:\n",
    "            raise RuntimeError(\"Failed to find focal length\")\n",
    "\n",
    "        # failed to find it in exif, use prior\n",
    "        FOCAL_PRIOR = 1.2\n",
    "        focal = FOCAL_PRIOR * max_size\n",
    "\n",
    "    return focal\n",
    "\n",
    "def create_camera(db, image_path, camera_model):\n",
    "    image         = Image.open(image_path)\n",
    "    width, height = image.size\n",
    "\n",
    "    focal = get_focal(image_path)\n",
    "\n",
    "    if camera_model == 'simple-pinhole':\n",
    "        model = 0 # simple pinhole\n",
    "        param_arr = np.array([focal, width / 2, height / 2])\n",
    "    if camera_model == 'pinhole':\n",
    "        model = 1 # pinhole\n",
    "        param_arr = np.array([focal, focal, width / 2, height / 2])\n",
    "    elif camera_model == 'simple-radial':\n",
    "        model = 2 # simple radial\n",
    "        param_arr = np.array([focal, width / 2, height / 2, 0.1])\n",
    "    elif camera_model == 'opencv':\n",
    "        model = 4 # opencv\n",
    "        param_arr = np.array([focal, focal, width / 2, height / 2, 0., 0., 0., 0.])\n",
    "         \n",
    "    return db.add_camera(model, width, height, param_arr)\n",
    "\n",
    "\n",
    "def add_keypoints(db, h5_path, image_path, img_ext, camera_model, single_camera = True):\n",
    "    keypoint_f = h5py.File(os.path.join(h5_path, 'keypoints.h5'), 'r')\n",
    "\n",
    "    camera_id = None\n",
    "    fname_to_id = {}\n",
    "    for filename in tqdm(list(keypoint_f.keys())):\n",
    "        keypoints = keypoint_f[filename][()]\n",
    "\n",
    "        fname_with_ext = filename# + img_ext\n",
    "        path = os.path.join(image_path, fname_with_ext)\n",
    "        if not os.path.isfile(path):\n",
    "            raise IOError(f'Invalid image path {path}')\n",
    "\n",
    "        if camera_id is None or not single_camera:\n",
    "            camera_id = create_camera(db, path, camera_model)\n",
    "        image_id = db.add_image(fname_with_ext, camera_id)\n",
    "        fname_to_id[filename] = image_id\n",
    "\n",
    "        db.add_keypoints(image_id, keypoints)\n",
    "\n",
    "    return fname_to_id\n",
    "\n",
    "def add_matches(db, h5_path, fname_to_id):\n",
    "    match_file = h5py.File(os.path.join(h5_path, 'matches.h5'), 'r')\n",
    "    \n",
    "    added = set()\n",
    "    n_keys = len(match_file.keys())\n",
    "    n_total = (n_keys * (n_keys - 1)) // 2\n",
    "\n",
    "    with tqdm(total=n_total) as pbar:\n",
    "        for key_1 in match_file.keys():\n",
    "            group = match_file[key_1]\n",
    "            for key_2 in group.keys():\n",
    "                id_1 = fname_to_id[key_1]\n",
    "                id_2 = fname_to_id[key_2]\n",
    "\n",
    "                pair_id = image_ids_to_pair_id(id_1, id_2)\n",
    "                if pair_id in added:\n",
    "                    warnings.warn(f'Pair {pair_id} ({id_1}, {id_2}) already added!')\n",
    "                    continue\n",
    "            \n",
    "                matches = group[key_2][()]\n",
    "                db.add_matches(id_1, id_2, matches)\n",
    "\n",
    "                added.add(pair_id)\n",
    "\n",
    "                pbar.update(1)\n",
    "                \n",
    "def import_into_colmap(img_dir,\n",
    "                       feature_dir ='.featureout',\n",
    "                       database_path = 'colmap.db',\n",
    "                       img_ext='.jpg'):\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, img_ext, CONFIG.CAMERA_MODEL, single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "\n",
    "    db.commit()\n",
    "    return\n",
    "\n",
    "from torchvision.io import read_image as T_read_image\n",
    "from torchvision.io import ImageReadMode\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# General utilities\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from fastprogress import progress_bar\n",
    "import gc\n",
    "import numpy as np\n",
    "import h5py\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# CV/ML\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "from PIL import Image\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from torchvision import transforms\n",
    "\n",
    "# We will use ViT global descriptor to get matching shortlists.\n",
    "#zhr: Here's the version of using different models, including the one model version and multiple model version(designed first).\n",
    "###################\n",
    "#Two model Version#\n",
    "###################\n",
    "\n",
    "# def get_global_desc(fnames, model, model2,\n",
    "#                     device =  device):\n",
    "#     model = model.eval()\n",
    "#     model = model.to(device)\n",
    "#     model2 = model2.eval()\n",
    "#     model2 = model2.to(device)\n",
    "#     config = resolve_data_config({}, model=model)\n",
    "#     transform = transforms.Compose([\n",
    "#                 transforms.Resize(600, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "#                 transforms.CenterCrop(600),\n",
    "#                 transforms.ToTensor(),\n",
    "#                 transforms.Normalize([0.4850, 0.4560, 0.4060], [0.2290, 0.2240, 0.2250]),])#create_transform(**config)\n",
    "#     global_descs_convnext=[]\n",
    "#     for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n",
    "#         key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "#         img = Image.open(img_fname_full).convert('RGB')\n",
    "#         timg = transform(img).unsqueeze(0).to(device)\n",
    "#         with torch.no_grad():\n",
    "#             desc = model.forward_features(timg.to(device)).mean(dim=(-1,2))#\n",
    "#             #descf = model.forward_features(timg.to(device).flip(3)).mean(dim=(-1,2))#\n",
    "#             desc2 = model2.forward_features(timg.to(device)).mean(dim=(-1,2))#\n",
    "#             #desc2f = model2.forward_features(timg.to(device).flip(3)).mean(dim=(-1,2))#\n",
    "#             #desc = desc+descf\n",
    "#             #desc2 = desc2+desc2f\n",
    "#             #print (desc.shape)\n",
    "#             desc = desc.view(1, -1)\n",
    "#             desc2 = desc2.view(1, -1)\n",
    "#             desc_norm = torch.cat([desc, desc2], dim=-1)\n",
    "#             desc_norm = F.normalize(desc_norm, dim=1, p=2)\n",
    "#         #print (desc_norm)\n",
    "#         global_descs_convnext.append(desc_norm.detach().cpu())\n",
    "#     global_descs_all = torch.cat(global_descs_convnext, dim=0)\n",
    "#     return global_descs_all\n",
    "\n",
    "\n",
    "#########################################################################################################################################\n",
    "\n",
    "\n",
    "def get_global_desc(fnames, model, device=device):\n",
    "    model = model.eval()\n",
    "    model = model.to(device)\n",
    "    config = resolve_data_config({}, model=model)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(600, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(600),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.4850, 0.4560, 0.4060], [0.2290, 0.2240, 0.2250]),\n",
    "    ])\n",
    "    \n",
    "    global_descs = []\n",
    "    for i, img_fname_full in tqdm(enumerate(fnames), total=len(fnames)):\n",
    "        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "        img = Image.open(img_fname_full).convert('RGB')\n",
    "        timg = transform(img).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            desc = model.forward_features(timg).mean(dim=(-1, 2))\n",
    "            desc = desc.view(1, -1)\n",
    "            desc_norm = F.normalize(desc, dim=1, p=2)\n",
    "        \n",
    "        global_descs.append(desc_norm.detach().cpu())\n",
    "    \n",
    "    global_descs_all = torch.cat(global_descs, dim=0)\n",
    "    return global_descs_all\n",
    "\n",
    "\n",
    "\n",
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs\n",
    "\n",
    "#########################\n",
    "## TWO MODELS UP       ##\n",
    "#########################\n",
    "\n",
    "# def get_image_pairs_shortlist_nn(fnames,\n",
    "#                               exhaustive_if_less = 20,\n",
    "#                               total_keypoints+=keypoints_countnneighbor=6,\n",
    "#                               device=torch.device('cpu')):\n",
    "#     num_imgs = len(fnames)\n",
    "#     if num_imgs <= exhaustive_if_less or num_imgs <= nneighbor :\n",
    "#         return get_img_pairs_exhaustive(fnames)\n",
    "\n",
    "#     model = timm.create_model('tf_efficientnet_b7',\n",
    "#                              checkpoint_path='/kaggle/input/tf-efficientnet/pytorch/tf-efficientnet-b7/1/tf_efficientnet_b7_ra-6c08e654.pth')\n",
    "#     # model = timm.create_model('mobilenetv3_rw')\n",
    "#     model.eval()\n",
    "#     model2 = timm.create_model('tf_efficientnet_b6',\n",
    "#                              checkpoint_path='/kaggle/input/tf-efficientnet/pytorch/tf-efficientnet-b6/1/tf_efficientnet_b6_aa-80ba17e4.pth')\n",
    "    \n",
    "#     # model2 = timm.create_model('resnet')\n",
    "#     #\n",
    "#     # model2 = timm.create_model('mobilenetv3_rw')\n",
    "#     model2.eval()\n",
    "#     descs = get_global_desc(fnames, model, model2, device=device)\n",
    "#     dm = torch.einsum('bi,ki->bk', descs, descs).detach().cpu()\n",
    "#     print(f\"sim shape: {dm.shape}\")\n",
    "    \n",
    "#     val, indices = torch.topk(dm, k=nneighbor, dim=1)\n",
    "\n",
    "#     matching_list = []\n",
    "#     for st_idx in range(num_imgs-1):\n",
    "#         for t in indices[st_idx][val[st_idx]>0.22]:\n",
    "#             if t == st_idx:\n",
    "#                 continue\n",
    "#             matching_list.append(tuple(sorted((st_idx, t.item()))))\n",
    "#     matching_list = sorted(list(set(matching_list)))\n",
    "#     del model, model2\n",
    "#     torch.cuda.empty_cache()\n",
    "#     return matching_list\n",
    "\n",
    "#########################\n",
    "## TWO MODELS LOW      ##\n",
    "#########################\n",
    "\n",
    "def get_image_pairs_shortlist_nn(fnames,\n",
    "                                 exhaustive_if_less=20,\n",
    "                                 nneighbor=6,\n",
    "                                 device=torch.device('cpu')):\n",
    "    num_imgs = len(fnames)\n",
    "    print(f\"{num_imgs}!!!!!!!!!!!\")\n",
    "    # If the number of images is small, use exhaustive search\n",
    "    if num_imgs <= exhaustive_if_less or num_imgs <= nneighbor:\n",
    "         print(\"GO!\")\n",
    "         return get_img_pairs_exhaustive(fnames)\n",
    "    start_time = time()\n",
    "    # Load the model (using a single model instead of two)\n",
    "    model = timm.create_model('resnet18',pretrained=False)\n",
    "    #model = timm.create_model('tf_efficientnet_b7',)\n",
    "    #                         checkpoint_path='/kaggle/input/tf-efficientnet/pytorch/tf-efficientnet-b7/1/tf_efficientnet_b7_ra-6c08e654.pth')\n",
    "    model.eval()\n",
    "    \n",
    "    # Get global descriptors using the single model\n",
    "    descs = get_global_desc(fnames, model, device=device)\n",
    "    \n",
    "    # Compute the similarity matrix\n",
    "    dm = torch.einsum('bi,ki->bk', descs, descs).detach().cpu()\n",
    "    print(f\"sim shape: {dm.shape}\")\n",
    "    \n",
    "    # Get the top-k nearest neighbors for each image\n",
    "    val, indices = torch.topk(dm, k=nneighbor, dim=1)\n",
    "\n",
    "    # Generate the matching list based on similarity\n",
    "    matching_list = []\n",
    "    for st_idx in range(num_imgs-1):\n",
    "        for t in indices[st_idx][val[st_idx] > 0.22]:\n",
    "            if t == st_idx:\n",
    "                continue\n",
    "            matching_list.append(tuple(sorted((st_idx, t.item()))))\n",
    "    \n",
    "    # Sort and remove duplicates from the matching list\n",
    "    matching_list = sorted(list(set(matching_list)))\n",
    "    end_time = time()\n",
    "    # Clean up to free memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    total_time=start_time-end_time\n",
    "    print(\"time!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    print(f\"The model uses:{total_time}\")\n",
    "\n",
    "    return matching_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_torch_image(fname, device=torch.device('cpu')):\n",
    "    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "    return img\n",
    "\n",
    "def convert_coord(r, w, h, rotk):\n",
    "    if rotk == 0:\n",
    "        return r\n",
    "    elif rotk == 1:\n",
    "        rx = w-1-r[:, 1]\n",
    "        ry = r[:, 0]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "    elif rotk == 2:\n",
    "        rx = w-1-r[:, 0]\n",
    "        ry = h-1-r[:, 1]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "    elif rotk == 3:\n",
    "        rx = r[:, 1]\n",
    "        ry = h-1-r[:, 0]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "\n",
    "def detect_common(img_fnames,\n",
    "                  model_name,\n",
    "                  rots,\n",
    "                  file_keypoints,\n",
    "                  feature_dir = '.featureout',\n",
    "                  num_features = 4096,\n",
    "                  resize_to = 1024,\n",
    "                  detection_threshold = 0.01,\n",
    "                  device=torch.device('cpu'),\n",
    "                  min_matches=15,verbose=True\n",
    "                 ):\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "\n",
    "    #####################################################\n",
    "    # Extract keypoints and descriptions\n",
    "    #####################################################\n",
    "    dict_model = {\n",
    "        \"aliked\" : ALIKED,\n",
    "        \"superpoint\" : SuperPoint,\n",
    "        \"doghardnet\" : DoGHardNet,\n",
    "        \"disk\" : DISK,\n",
    "        \"sift\" : SIFT,\n",
    "    }\n",
    "    extractor_class = dict_model[model_name]\n",
    "    \n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    extractor = extractor_class(\n",
    "        max_num_keypoints=num_features, detection_threshold=detection_threshold, resize=resize_to\n",
    "    ).eval().to(device, dtype)\n",
    "        \n",
    "    dict_kpts_cuda = {}\n",
    "    dict_descs_cuda = {}\n",
    "    dict_kpts_rot_cuda = {}\n",
    "    total_key=0\n",
    "\n",
    "\n",
    "    for (img_path, rot_k) in zip(img_fnames, rots):\n",
    "        img_fname = img_path.split('/')[-1]\n",
    "        key = img_fname\n",
    "        with torch.inference_mode():\n",
    "            image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "            h, w = image0.shape[2], image0.shape[3]\n",
    "            image1 = torch.rot90(image0, rot_k, [2, 3])\n",
    "            feats0 = extractor.extract(image1)  # auto-resize the image, disable with resize=None\n",
    "            kpts_rot = feats0['keypoints'].reshape(-1, 2).detach()\n",
    "            descs = feats0['descriptors'].reshape(len(kpts_rot), -1).detach()\n",
    "            dict_kpts_rot_cuda[f\"{key}\"] = kpts_rot\n",
    "            \n",
    "            kpts = convert_coord(kpts_rot, w, h, rot_k)\n",
    "            dict_kpts_cuda[f\"{key}\"] = kpts\n",
    "            dict_descs_cuda[f\"{key}\"] = descs\n",
    "\n",
    "\n",
    "            total_key+=kpts.shape[0]\n",
    "            print(f\"{model_name} > rot_k={rot_k}, kpts.shape={kpts.shape}, descs.shape={descs.shape}\")\n",
    "    del extractor\n",
    "    gc.collect()\n",
    "\n",
    "    #####################################################\n",
    "    # Matching keypoints\n",
    "    #####################################################\n",
    "    lg_matcher = KF.LightGlueMatcher(model_name, {\"width_confidence\": -1,\n",
    "                                            \"depth_confidence\": -1,\n",
    "                                             \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n",
    "    \n",
    "    cnt_pairs = 0\n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:\n",
    "        for pair_idx in tqdm(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            \n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "            \n",
    "            kp1_rot = dict_kpts_rot_cuda[key1]\n",
    "            kp2_rot = dict_kpts_rot_cuda[key2]\n",
    "            \n",
    "            kp1 = dict_kpts_cuda[key1]\n",
    "            kp2 = dict_kpts_cuda[key2]\n",
    "            \n",
    "            desc1 = dict_descs_cuda[key1]\n",
    "            desc2 = dict_descs_cuda[key2]\n",
    "            with torch.inference_mode():\n",
    "                dists, idxs = lg_matcher(desc1,\n",
    "                                     desc2,\n",
    "                                     KF.laf_from_center_scale_ori(kp1_rot[None]),\n",
    "                                     KF.laf_from_center_scale_ori(kp2_rot[None]))\n",
    "            if len(idxs)  == 0:\n",
    "                continue\n",
    "            n_matches = len(idxs)\n",
    "            kp1 = kp1[idxs[:,0], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "            kp2 = kp2[idxs[:,1], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=np.concatenate([kp1, kp2], axis=1))\n",
    "                cnt_pairs+=1\n",
    "                print (f'{model_name}> {key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair({model_name}+lightglue)')            \n",
    "            else:\n",
    "                print (f'{model_name}> {key1}-{key2}: {n_matches} matches --> skipped')\n",
    "    del lg_matcher\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return total_key\n",
    "\n",
    "def detect_lightglue_common(\n",
    "    img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "    resize_to=1024,\n",
    "    detection_threshold=0.01, \n",
    "    num_features=4096, \n",
    "    min_matches=15,\n",
    "):\n",
    "    t=time()\n",
    "    total_key=detect_common(\n",
    "        img_fnames, model_name, rots, file_keypoints, feature_dir, \n",
    "        resize_to=resize_to,\n",
    "        num_features=num_features, \n",
    "        detection_threshold=detection_threshold, \n",
    "        device=device,\n",
    "        min_matches=min_matches,\n",
    "    )\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec ({model_name}+LightGlue)')\n",
    "    return t,total_key\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/zhr/user/3D-reconstruction/kaggle/input/super-glue-pretrained-network\")\n",
    "from models.matching import Matching\n",
    "from models.superpoint import SuperPoint as SG_SuperPoint\n",
    "from models.superglue import SuperGlue\n",
    "from models.utils import (compute_pose_error, compute_epipolar_error,\n",
    "                          estimate_pose, make_matching_plot,\n",
    "                          error_colormap, AverageTimer, pose_auc, read_image,\n",
    "                          process_resize, frame2tensor,\n",
    "                          rotate_intrinsics, rotate_pose_inplane,\n",
    "                          scale_intrinsics)\n",
    "\n",
    "from torch.nn import functional as torchF  # For resizing tensor\n",
    "\n",
    "# Preprocess\n",
    "def sg_read_image(image, device, resize):\n",
    "    w, h = image.shape[1], image.shape[0]\n",
    "    w_new, h_new = process_resize(w, h, [resize,])\n",
    "    \n",
    "    unit_shape = 8\n",
    "    w_new = w_new // unit_shape * unit_shape\n",
    "    h_new = h_new // unit_shape * unit_shape\n",
    "    \n",
    "    scales = (float(w) / float(w_new), float(h) / float(h_new))\n",
    "    image = cv2.resize(image.astype('float32'), (w_new, h_new))\n",
    "\n",
    "    inp = frame2tensor(image, \"cpu\")\n",
    "    return image, inp, scales, (h, w)\n",
    "\n",
    "class SGDataset(Dataset):\n",
    "    def __init__(self, img_fnames, resize_to, device):\n",
    "        self.img_fnames = img_fnames\n",
    "        self.resize_to = resize_to\n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_fnames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.img_fnames[idx]\n",
    "        im = cv2.imread(fname, cv2.IMREAD_GRAYSCALE)\n",
    "        _, image, scale, ori_shape = sg_read_image(im, self.device, self.resize_to)\n",
    "        return image, torch.tensor([idx]), torch.tensor(ori_shape)\n",
    "\n",
    "def get_superglue_dataloader(img_fnames, resize_to, device, batch_size=1):\n",
    "    dataset = SGDataset(img_fnames, resize_to, device)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def detect_superglue(\n",
    "    img_fnames, index_pairs, feature_dir, device, sg_config, file_keypoints, file_keypoints_crop,\n",
    "    resize_to=750, min_matches=15\n",
    "):    \n",
    "    t=time()\n",
    "    total_key=0\n",
    "\n",
    "    fnames1, fnames2, idxs1, idxs2 = [], [], [], []\n",
    "    for pair_idx in progress_bar(index_pairs):\n",
    "        idx1, idx2 = pair_idx\n",
    "        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "        fnames1.append(fname1)\n",
    "        fnames2.append(fname2)\n",
    "        idxs1.append(idx1)\n",
    "        idxs2.append(idx2)\n",
    "        \n",
    "    dataloader = get_superglue_dataloader(img_fnames=img_fnames, resize_to=1680, device=device)\n",
    "\n",
    "    #####################################################\n",
    "    # Extract keypoints and descriptions\n",
    "    #####################################################\n",
    "    superpoint = SG_SuperPoint(sg_config[\"superpoint\"]).eval().to(device)\n",
    "    dict_features_cuda = {}\n",
    "    dict_shapes = {}\n",
    "    dict_images = {}\n",
    "    #dict_fname_shapes = {}\n",
    "    for X in dataloader:\n",
    "        image, idx, ori_shape = X\n",
    "        image = image[0].to(device)\n",
    "        fname = img_fnames[idx]\n",
    "        #dict_fname_shapes[fname] = ori_shape\n",
    "        key = fname.split('/')[-1]\n",
    "        \n",
    "        with torch.no_grad(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            pred = superpoint({'image': image})\n",
    "            dict_features_cuda[key] = pred\n",
    "            dict_shapes[key] = ori_shape\n",
    "            dict_images[key] = image.half()\n",
    "\n",
    "            if isinstance(pred[\"keypoints\"], list):\n",
    "                for kp in pred[\"keypoints\"]:\n",
    "                    total_key += kp.shape[0]  # Assuming kp is a tensor\n",
    "            elif isinstance(pred[\"keypoints\"], torch.Tensor):\n",
    "                total_key += pred[\"keypoints\"].shape[0]\n",
    "\n",
    "    del superpoint\n",
    "    gc.collect()\n",
    "    \n",
    "    #####################################################\n",
    "    # Matching keypoints\n",
    "    #####################################################\n",
    "    superglue = SuperGlue(sg_config[\"superglue\"]).eval().to(device)\n",
    "    weights = sg_config[\"superglue\"][\"weights\"]\n",
    "    cnt_pairs = 0\n",
    "    \n",
    "    mkpt = {}\n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:\n",
    "        for idx, (fname1, fname2) in enumerate(zip(fnames1, fnames2)):\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "\n",
    "            data = {\"image0\": dict_images[key1], \"image1\": dict_images[key2]}\n",
    "            data = {**data, **{k+'0': v for k, v in dict_features_cuda[key1].items()}}\n",
    "            data = {**data, **{k+'1': v for k, v in dict_features_cuda[key2].items()}}\n",
    "            for k in data:\n",
    "                if isinstance(data[k], (list, tuple)):\n",
    "                    data[k] = torch.stack(data[k])\n",
    "            with torch.no_grad(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                pred = {**data, **superglue(data)}\n",
    "                pred = {k: v[0].detach().cpu().numpy().copy() for k, v in pred.items()}\n",
    "            mkpts1, mkpts2 = pred[\"keypoints0\"], pred[\"keypoints1\"]\n",
    "            matches, conf = pred[\"matches0\"], pred[\"matching_scores0\"]\n",
    "\n",
    "            valid = matches > -1\n",
    "            mkpts1 = mkpts1[valid]\n",
    "            mkpts2 = mkpts2[matches[valid]]\n",
    "            mconf = conf[valid]\n",
    "\n",
    "            ori_shape_1 = dict_shapes[key1][0].numpy()\n",
    "            ori_shape_2 = dict_shapes[key2][0].numpy()\n",
    "            \n",
    "            # Scaling coords\n",
    "            mkpts1[:,0] = mkpts1[:,0] * ori_shape_1[1] / dict_images[key1].shape[3]   # X\n",
    "            mkpts1[:,1] = mkpts1[:,1] * ori_shape_1[0] / dict_images[key1].shape[2]   # Y\n",
    "            mkpts2[:,0] = mkpts2[:,0] * ori_shape_2[1] / dict_images[key2].shape[3]   # X\n",
    "            mkpts2[:,1] = mkpts2[:,1] * ori_shape_2[0] / dict_images[key2].shape[2]   # Y  \n",
    "            \n",
    "            n_matches = mconf.shape[0]\n",
    "            \n",
    "            if fname1 in mkpt:\n",
    "                mkpt[fname1] = np.concatenate([mkpt[fname1], mkpts1], axis=0).astype(np.float32)\n",
    "            else:\n",
    "                mkpt[fname1] = mkpts1\n",
    "            if fname2 in mkpt:\n",
    "                mkpt[fname2] = np.concatenate([mkpt[fname2], mkpts2], axis=0).astype(np.float32)\n",
    "            else:\n",
    "                mkpt[fname2] = mkpts2\n",
    "            \n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=np.concatenate([mkpts1, mkpts2], axis=1).astype(np.float32))\n",
    "                cnt_pairs+=1\n",
    "                print (f'{key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair(superglue/{resize_to}/{weights})')            \n",
    "            else:\n",
    "                print (f'{key1}-{key2}: {n_matches} matches --> skipped')\n",
    "    \n",
    "       \n",
    "    \n",
    "    gc.collect()\n",
    "    del superglue\n",
    "    del dict_features_cuda\n",
    "    del dict_images\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec')\n",
    "    return t,total_key\n",
    "\n",
    "# Making kornia local features loading w/o internet\n",
    "class AffNetHardNet(KF.LocalFeature):\n",
    "    \"\"\"Convenience module, which implements KeyNet detector + AffNet + HardNet descriptor.\n",
    "\n",
    "    .. image:: _static/img/keynet_affnet.jpg\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int = 5000,\n",
    "        upright: bool = False,\n",
    "        device = torch.device('cpu'),\n",
    "        scale_laf: float = 1.0,\n",
    "        detector_type='GFTT',\n",
    "    ):\n",
    "        config = {\n",
    "            # Extraction Parameters\n",
    "            \"nms_size\": 15,\n",
    "            \"pyramid_levels\": 4,\n",
    "            \"up_levels\": 1,\n",
    "            \"scale_factor_levels\": math.sqrt(2),\n",
    "            \"s_mult\": 22.0,\n",
    "        }\n",
    "        ori_module = KF.PassLAF() if upright else KF.LAFOrienter(angle_detector=KF.OriNet(False)).eval()\n",
    "        if not upright:\n",
    "            weights = torch.load('/home/zhr/user/3D-reconstruction/kaggle/input/kornia-local-feature-weights/OriNet.pth')['state_dict']\n",
    "            ori_module.angle_detector.load_state_dict(weights)\n",
    "        #detector = KF.KeyNetDetector(\n",
    "        #    False, num_features=num_features, ori_module=ori_module, aff_module=KF.LAFAffNetShapeEstimator(False).eval()\n",
    "        #).to(device)\n",
    "        if detector_type=='GFTT':\n",
    "            detector = KF.MultiResolutionDetector(\n",
    "                    KF.CornerGFTT(),\n",
    "                    num_features=num_features,\n",
    "                    config=config,\n",
    "                    ori_module=ori_module,\n",
    "                    aff_module=KF.LAFAffNetShapeEstimator(False).eval(),\n",
    "                ).to(device)\n",
    "        elif detector_type=='DoG':\n",
    "            detector = KF.MultiResolutionDetector(\n",
    "                    KF.BlobDoGSingle(),\n",
    "                    num_features=num_features,\n",
    "                    config=config,\n",
    "                    ori_module=ori_module,\n",
    "                    aff_module=KF.LAFAffNetShapeEstimator(False).eval(),\n",
    "                ).to(device)\n",
    "        else:\n",
    "            detector = KF.MultiResolutionDetector(\n",
    "                    KF.CornerHarris(0.04),\n",
    "                    num_features=num_features,\n",
    "                    config=config,\n",
    "                    ori_module=ori_module,\n",
    "                    aff_module=KF.LAFAffNetShapeEstimator(False).eval(),\n",
    "                ).to(device)\n",
    "        #kn_weights = torch.load('/kaggle/input/kornia-local-feature-weights/keynet_pytorch.pth')['state_dict']\n",
    "        #detector.model.load_state_dict(kn_weights)\n",
    "        affnet_weights = torch.load('//home/zhr/user/3D-reconstruction/kaggle/input/kornia-local-feature-weights/AffNet.pth')['state_dict']\n",
    "        detector.aff.load_state_dict(affnet_weights)\n",
    "        \n",
    "        hardnet = KF.HardNet(False).eval()\n",
    "        hn_weights = torch.load('/home/zhr/user/3D-reconstruction/kaggle/input/kornia-local-feature-weights/HardNetLib.pth')['state_dict']\n",
    "        hardnet.load_state_dict(hn_weights)\n",
    "        descriptor = KF.LAFDescriptor(hardnet, patch_size=32, grayscale_descriptor=True).to(device)\n",
    "        super().__init__(detector, descriptor, scale_laf)\n",
    "\n",
    "# Making kornia local features loading w/o internet\n",
    "class KeyNetAffNetHardNet(KF.LocalFeature):\n",
    "    \"\"\"Convenience module, which implements KeyNet detector + AffNet + HardNet descriptor.\n",
    "\n",
    "    .. image:: _static/img/keynet_affnet.jpg\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int = 5000,\n",
    "        upright: bool = False,\n",
    "        device = torch.device('cpu'),\n",
    "        scale_laf: float = 1.0,\n",
    "        \n",
    "    ):\n",
    "        ori_module = KF.PassLAF() if upright else KF.LAFOrienter(angle_detector=KF.OriNet(False)).eval()\n",
    "        if not upright:\n",
    "            weights = torch.load('/home/zhr/user/3D-reconstruction/kaggle/input/kornia-local-feature-weights/OriNet.pth')['state_dict']\n",
    "            ori_module.angle_detector.load_state_dict(weights)\n",
    "        detector = KF.KeyNetDetector(\n",
    "            False, num_features=num_features, ori_module=ori_module, aff_module=KF.LAFAffNetShapeEstimator(False).eval()\n",
    "        ).to(device)\n",
    "        kn_weights = torch.load('/home/zhr/user/3D-reconstruction/kaggle/input/kornia-local-feature-weights/keynet_pytorch.pth')['state_dict']\n",
    "        detector.model.load_state_dict(kn_weights)\n",
    "        affnet_weights = torch.load('/home/zhr/user/3D-reconstruction/kaggle/input/kornia-local-feature-weights/AffNet.pth')['state_dict']\n",
    "        detector.aff.load_state_dict(affnet_weights)\n",
    "        \n",
    "        hardnet = KF.HardNet(False).eval()\n",
    "        hn_weights = torch.load('/home/zhr/user/3D-reconstruction/kaggle/input/kornia-local-feature-weights/HardNetLib.pth')['state_dict']\n",
    "        hardnet.load_state_dict(hn_weights)\n",
    "        descriptor = KF.LAFDescriptor(hardnet, patch_size=32, grayscale_descriptor=True).to(device)\n",
    "        super().__init__(detector, descriptor, scale_laf)\n",
    "\n",
    "\n",
    "def detect_features(img_fnames,\n",
    "                    num_feats = 8000,\n",
    "                    upright = False,\n",
    "                    device=torch.device('cpu'),\n",
    "                    feature_dir = '.featureout',\n",
    "                    resize_small_edge_to = 1200,\n",
    "                   local_feature='Keynet'):\n",
    "\n",
    "    if local_feature == 'Keynet':\n",
    "        feature = KeyNetAffNetHardNet(num_feats, upright, device).to(device).eval()\n",
    "    else:\n",
    "        feature = AffNetHardNet(num_feats, upright, device, detector_type=local_feature).to(device).eval()\n",
    "    print('local feature:', local_feature)\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "    \n",
    "    total_key=0\n",
    "\n",
    "    with h5py.File(f'{feature_dir}/lafs.h5', mode='w') as f_laf, \\\n",
    "         h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors.h5', mode='w') as f_desc:\n",
    "        for img_path in progress_bar(img_fnames):\n",
    "            img_fname = img_path.split('/')[-1]\n",
    "            key = img_fname\n",
    "            with torch.inference_mode():\n",
    "                timg = load_torch_image(img_path, device=device)\n",
    "                H, W = timg.shape[2:]\n",
    "                if resize_small_edge_to is None:\n",
    "                    timg_resized = timg\n",
    "                else:\n",
    "                    timg_resized = K.geometry.resize(timg, resize_small_edge_to, antialias=True)\n",
    "                    print(f'Resized {timg.shape} to {timg_resized.shape} (resize_small_edge_to={resize_small_edge_to})')\n",
    "                h, w = timg_resized.shape[2:]\n",
    "                \n",
    "                lafs, resps, descs = feature(K.color.rgb_to_grayscale(timg_resized))\n",
    "                lafs[:,:,0,:] *= float(W) / float(w)\n",
    "                lafs[:,:,1,:] *= float(H) / float(h)\n",
    "                desc_dim = descs.shape[-1]\n",
    "                kpts = KF.get_laf_center(lafs).reshape(-1, 2).detach().cpu().numpy()\n",
    "                descs = descs.reshape(-1, desc_dim).detach().cpu().numpy()\n",
    "                \n",
    "                total_key+=kpts.shape[0]\n",
    "\n",
    "                f_laf[key] = lafs.detach().cpu().numpy()\n",
    "                f_kp[key] = kpts\n",
    "                f_desc[key] = descs\n",
    "    return total_key\n",
    "\n",
    "def match_features(img_fnames,\n",
    "                   index_pairs,\n",
    "                   file_keypoints,\n",
    "                   feature_dir = '.featureout',\n",
    "                   device=torch.device('cpu'),\n",
    "                   min_matches=15, \n",
    "                   force_mutual = True,\n",
    "                   matching_alg='adalam'\n",
    "                  ):\n",
    "    assert matching_alg in ['smnn', 'adalam']\n",
    "    with h5py.File(f'{feature_dir}/lafs.h5', mode='r') as f_laf, \\\n",
    "        h5py.File(f'{feature_dir}/keypoints.h5', mode='r') as f_kp, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc, \\\n",
    "        h5py.File(file_keypoints, mode='w') as f_match:\n",
    "\n",
    "        for pair_idx in progress_bar(index_pairs):\n",
    "                    idx1, idx2 = pair_idx\n",
    "                    fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "                    key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "                    lafs1 = torch.from_numpy(f_laf[key1][...]).to(device)\n",
    "                    lafs2 = torch.from_numpy(f_laf[key2][...]).to(device)\n",
    "                    desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n",
    "                    desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n",
    "                    kp1 = torch.from_numpy(f_kp[key1][...]).to(device)\n",
    "                    kp2 = torch.from_numpy(f_kp[key2][...]).to(device)\n",
    "                    if matching_alg == 'adalam':\n",
    "                        img1, img2 = cv2.imread(fname1), cv2.imread(fname2)\n",
    "                        hw1, hw2 = img1.shape[:2], img2.shape[:2]\n",
    "                        adalam_config = KF.adalam.get_adalam_default_config()\n",
    "                        #adalam_config['orientation_difference_threshold'] = None\n",
    "                        #adalam_config['scale_rate_threshold'] = None\n",
    "                        adalam_config['force_seed_mnn']= True\n",
    "                        adalam_config['search_expansion'] = 16\n",
    "                        adalam_config['ransac_iters'] = 128\n",
    "                        adalam_config['device'] = device\n",
    "                        dists, idxs = KF.match_adalam(desc1, desc2,\n",
    "                                                      lafs1, lafs2, # Adalam takes into account also geometric information\n",
    "                                                      hw1=hw1, hw2=hw2,\n",
    "                                                      config=adalam_config) # Adalam also benefits from knowing image size\n",
    "                    else:\n",
    "                        dists, idxs = KF.match_smnn(desc1, desc2, 0.98)\n",
    "                    if len(idxs)  == 0:\n",
    "                        continue\n",
    "                    # Force mutual nearest neighbors\n",
    "                    if dists.mean().cpu().numpy() < 0.05:\n",
    "                        first_indices = get_unique_idxs(idxs[:,1])\n",
    "                        idxs = idxs[first_indices]\n",
    "                        dists = dists[first_indices]\n",
    "                    n_matches = len(idxs)\n",
    "                    kp1 = kp1[idxs[:,0], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "                    kp2 = kp2[idxs[:,1], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "                    if False:\n",
    "                        print (f'{key1}-{key2}: {n_matches} matches')\n",
    "                    group  = f_match.require_group(key1)\n",
    "                    if n_matches >= min_matches:\n",
    "                         group.create_dataset(key2, data=np.concatenate([kp1, kp2], axis=1))\n",
    "    return\n",
    "\n",
    "\n",
    "def get_unique_idxs(A, dim=0):\n",
    "    # https://stackoverflow.com/questions/72001505/how-to-get-unique-elements-and-their-firstly-appeared-indices-of-a-pytorch-tenso\n",
    "    unique, idx, counts = torch.unique(A, dim=dim, sorted=True, return_inverse=True, return_counts=True)\n",
    "    _, ind_sorted = torch.sort(idx, stable=True)\n",
    "    cum_sum = counts.cumsum(0)\n",
    "    cum_sum = torch.cat((torch.tensor([0],device=cum_sum.device), cum_sum[:-1]))\n",
    "    first_indices = ind_sorted[cum_sum]\n",
    "    return first_indices\n",
    "\n",
    "def get_keypoint_from_h5(fp, key1, key2):\n",
    "    rc = -1\n",
    "    try:\n",
    "        kpts = np.array(fp[key1][key2])\n",
    "        rc = 0\n",
    "        return (rc, kpts)\n",
    "    except:\n",
    "        return (rc, None)\n",
    "\n",
    "def get_keypoint_from_multi_h5(fps, key1, key2):\n",
    "    list_mkpts = []\n",
    "    for fp in fps:\n",
    "        rc, mkpts = get_keypoint_from_h5(fp, key1, key2)\n",
    "        if rc == 0:\n",
    "            list_mkpts.append(mkpts)\n",
    "    if len(list_mkpts) > 0:\n",
    "        list_mkpts = np.concatenate(list_mkpts, axis=0)\n",
    "    else:\n",
    "        list_mkpts = None\n",
    "    return list_mkpts\n",
    "\n",
    "def matches_merger(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    files_keypoints,\n",
    "    save_file,\n",
    "    feature_dir = 'featureout',\n",
    "    filter_FundamentalMatrix = False,\n",
    "    filter_iterations = 10,\n",
    "    filter_threshold = 8,\n",
    "):\n",
    "    # open h5 files\n",
    "    fps = [ h5py.File(file, mode=\"r\") for file in files_keypoints ]\n",
    "    total_match_count=0 #####################\n",
    "\n",
    "    with h5py.File(save_file, mode='w') as f_match:\n",
    "        counter = 0\n",
    "        for pair_idx in progress_bar(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "\n",
    "            # extract keypoints\n",
    "            mkpts = get_keypoint_from_multi_h5(fps, key1, key2)\n",
    "            if mkpts is None:\n",
    "                print(f\"skipped key1={key1}, key2={key2}\")\n",
    "                continue\n",
    "\n",
    "            ori_size = mkpts.shape[0]\n",
    "            if mkpts.shape[0] < CONFIG.MERGE_PARAMS[\"min_matches\"]:\n",
    "                continue\n",
    "            \n",
    "            if filter_FundamentalMatrix:\n",
    "                store_inliers = { idx:0 for idx in range(mkpts.shape[0]) }\n",
    "                idxs = np.array(range(mkpts.shape[0]))\n",
    "                for iter in range(filter_iterations):\n",
    "                    try:\n",
    "                        Fm, inliers = cv2.findFundamentalMat(\n",
    "                            mkpts[:,:2], mkpts[:,2:4], cv2.USAC_MAGSAC, 0.15, 0.9999, 20000)\n",
    "                        if Fm is not None:\n",
    "                            inliers = inliers > 0\n",
    "                            inlier_idxs = idxs[inliers[:, 0]]\n",
    "                            #print(inliers.shape, inlier_idxs[:5])\n",
    "                            for idx in inlier_idxs:\n",
    "                                store_inliers[idx] += 1\n",
    "                    except:\n",
    "                        print(f\"Failed to cv2.findFundamentalMat. mkpts.shape={mkpts.shape}\")\n",
    "                inliers = np.array([ count for (idx, count) in store_inliers.items() ]) >= filter_threshold\n",
    "                mkpts = mkpts[inliers]\n",
    "                if mkpts.shape[0] < 15:\n",
    "                    print(f\"skipped key1={key1}, key2={key2}: mkpts.shape={mkpts.shape} after filtered.\")\n",
    "                    continue\n",
    "                #print(f\"filter_FundamentalMatrix: {len(store_inliers)} matches --> {mkpts.shape[0]} matches\")\n",
    "            total_match_count +=mkpts.shape[0] ###########################\n",
    "            \n",
    "            print (f'{key1}-{key2}: {ori_size} --> {mkpts.shape[0]} matches')            \n",
    "            # regist tmp file\n",
    "            group  = f_match.require_group(key1)\n",
    "            group.create_dataset(key2, data=mkpts)\n",
    "            counter += 1\n",
    "    print( f\"Ensembled pairs : {counter} pairs\" )\n",
    "    for fp in fps:\n",
    "        fp.close()\n",
    "    return total_match_count\n",
    "\n",
    "\n",
    "\n",
    "def keypoints_merger(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    files_keypoints,\n",
    "    feature_dir = 'featureout',\n",
    "    filter_FundamentalMatrix = False,\n",
    "    filter_iterations = 10,\n",
    "    filter_threshold = 8,\n",
    "):\n",
    "    save_file = f'{feature_dir}/merge_tmp.h5'\n",
    "    !rm -rf {save_file}\n",
    "    print(\"#####################################################################\")\n",
    "    print(\"MERGE BEGIN\")\n",
    "    match_count=matches_merger(\n",
    "        img_fnames,\n",
    "        index_pairs,\n",
    "        files_keypoints,\n",
    "        save_file,\n",
    "        feature_dir = feature_dir,\n",
    "        filter_FundamentalMatrix = filter_FundamentalMatrix,\n",
    "        filter_iterations = filter_iterations,\n",
    "        filter_threshold = filter_threshold,\n",
    "    )\n",
    "    #print(f\"Total matches count:{match_count} matches\")    \n",
    "    # Let's find unique loftr pixels and group them together.\n",
    "    kpts = defaultdict(list)\n",
    "    match_indexes = defaultdict(dict)\n",
    "    total_kpts=defaultdict(int)\n",
    "    total_keypoints_count=0 ###################\n",
    "    with h5py.File(save_file, mode='r') as f_match:\n",
    "        for k1 in f_match.keys():\n",
    "            group  = f_match[k1]\n",
    "            for k2 in group.keys():\n",
    "                matches = group[k2][...]\n",
    "                total_kpts[k1]\n",
    "                kpts[k1].append(matches[:, :2])\n",
    "                kpts[k2].append(matches[:, 2:])\n",
    "                current_match = torch.arange(len(matches)).reshape(-1, 1).repeat(1, 2)\n",
    "                current_match[:, 0]+=total_kpts[k1]\n",
    "                current_match[:, 1]+=total_kpts[k2]\n",
    "                total_kpts[k1]+=len(matches)\n",
    "                total_kpts[k2]+=len(matches)\n",
    "                match_indexes[k1][k2]=current_match\n",
    "                total_keypoints_count+=matches.shape[0]*2\n",
    "    #print(f\"Total Key Points:{total_keypoints_count} keypoints\")\n",
    "    for k in kpts.keys():\n",
    "        kpts[k] = np.round(np.concatenate(kpts[k], axis=0))\n",
    "    unique_kpts = {}\n",
    "    unique_match_idxs = {}\n",
    "    out_match = defaultdict(dict)\n",
    "    for k in kpts.keys():\n",
    "        uniq_kps, uniq_reverse_idxs = torch.unique(torch.from_numpy(kpts[k]),dim=0, return_inverse=True)\n",
    "        unique_match_idxs[k] = uniq_reverse_idxs\n",
    "        unique_kpts[k] = uniq_kps.numpy()\n",
    "    for k1, group in match_indexes.items():\n",
    "        for k2, m in group.items():\n",
    "            m2 = deepcopy(m)\n",
    "            m2[:,0] = unique_match_idxs[k1][m2[:,0]]\n",
    "            m2[:,1] = unique_match_idxs[k2][m2[:,1]]\n",
    "            mkpts = np.concatenate([unique_kpts[k1][ m2[:,0]],\n",
    "                                    unique_kpts[k2][  m2[:,1]],\n",
    "                                   ],\n",
    "                                   axis=1)\n",
    "            unique_idxs_current = get_unique_idxs(torch.from_numpy(mkpts), dim=0)\n",
    "            m2_semiclean = m2[unique_idxs_current]\n",
    "            unique_idxs_current1 = get_unique_idxs(m2_semiclean[:, 0], dim=0)\n",
    "            m2_semiclean = m2_semiclean[unique_idxs_current1]\n",
    "            unique_idxs_current2 = get_unique_idxs(m2_semiclean[:, 1], dim=0)\n",
    "            m2_semiclean2 = m2_semiclean[unique_idxs_current2]\n",
    "            out_match[k1][k2] = m2_semiclean2.numpy()\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp:\n",
    "        for k, kpts1 in unique_kpts.items():\n",
    "            f_kp[k] = kpts1\n",
    "    \n",
    "    with h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "        for k1, gr in out_match.items():\n",
    "            group  = f_match.require_group(k1)\n",
    "            for k2, match in gr.items():\n",
    "                group[k2] = match\n",
    "    return\n",
    "\n",
    "# zhr: for the lofter model, just a try.\n",
    "def detect_loftr(\n",
    "    img_fnames, index_pairs, feature_dir, device, params, file_keypoints,\n",
    "):\n",
    "    #from loftr import LoFTR\n",
    "    import torch\n",
    "\n",
    "    matcher = LoFTR(pretrained='outdoor').to(device).eval()\n",
    "\n",
    "    # Load and preprocess images\n",
    "    image_tensors = {}\n",
    "    for img_path in img_fnames:\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img_tensor = torch.from_numpy(img / 255.).float()[None, None].to(device)\n",
    "        image_tensors[img_path] = img_tensor\n",
    "\n",
    "    # Perform matching\n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:\n",
    "        for idx1, idx2 in tqdm(index_pairs):\n",
    "            img1_path = img_fnames[idx1]\n",
    "            img2_path = img_fnames[idx2]\n",
    "            key1 = os.path.basename(img1_path)\n",
    "            key2 = os.path.basename(img2_path)\n",
    "\n",
    "            batch = {\n",
    "                'image0': image_tensors[img1_path],\n",
    "                'image1': image_tensors[img2_path]\n",
    "            }\n",
    "\n",
    "            with torch.no_grad():\n",
    "                matcher(batch)\n",
    "\n",
    "            mkpts0 = batch['keypoints0'].cpu().numpy()\n",
    "            mkpts1 = batch['keypoints1'].cpu().numpy()\n",
    "\n",
    "            n_matches = mkpts0.shape[0]\n",
    "\n",
    "            if n_matches >= params['min_matches']:\n",
    "                group = f_match.require_group(key1)\n",
    "                group.create_dataset(key2, data=np.hstack((mkpts0, mkpts1)))\n",
    "                print(f'{key1}-{key2}: {n_matches} matches')\n",
    "            else:\n",
    "                print(f'{key1}-{key2}: {n_matches} matches --> skipped')\n",
    "\n",
    "def wrapper_keypoints(\n",
    "    img_fnames, index_pairs, feature_dir, device, timings, rots\n",
    "):\n",
    "    #############################################################\n",
    "    # get keypoints\n",
    "    #############################################################\n",
    "    files_keypoints = []\n",
    "    total_keypoints=0\n",
    "    \n",
    "    local_feature_model = 'GFTT' #Keynet GFTT DoG Harris\n",
    "    keypoints_count=detect_features(img_fnames, \n",
    "                    20000,\n",
    "                    feature_dir=local_feature_model,\n",
    "                    upright=False,\n",
    "                    device=device,\n",
    "                    resize_small_edge_to=600,\n",
    "                    local_feature=local_feature_model,#GFTT #DoG #Harris\n",
    "                               )\n",
    "    total_keypoints+=keypoints_count\n",
    "    #print(\"!!!!!!!!!\")\n",
    "    print(f\"keypoints_for local feature model:{total_keypoints}\")\n",
    "\n",
    "    file_keypoints = f\"{feature_dir}/matches_{local_feature_model}.h5\"\n",
    "    match_features(img_fnames, index_pairs, file_keypoints, feature_dir=local_feature_model,device=device)\n",
    "    files_keypoints.append( file_keypoints )\n",
    "    keypoints_count =0\n",
    "    \n",
    "    if CONFIG.use_superglue:\n",
    "        for params_sg in CONFIG.params_sgs:\n",
    "            resize_to = params_sg[\"resize_to\"]\n",
    "            file_keypoints = f\"{feature_dir}/matches_superglue_{resize_to}pix.h5\"\n",
    "            file_keypoints_crop = f\"{feature_dir}/matches_superglue_{resize_to}pix_crop.h5\"\n",
    "            !rm -rf {file_keypoints}\n",
    "            t,keypoints_count = detect_superglue(\n",
    "                img_fnames, index_pairs, feature_dir, device, \n",
    "                params_sg[\"sg_config\"], file_keypoints, file_keypoints_crop,\n",
    "                resize_to=params_sg[\"resize_to\"], \n",
    "                min_matches=params_sg[\"min_matches\"],\n",
    "            )\n",
    "            total_keypoints+=keypoints_count\n",
    "            gc.collect()\n",
    "            files_keypoints.append( file_keypoints )\n",
    "            #files_keypoints.append( file_keypoints_crop )\n",
    "            timings['feature_matching'].append(t)\n",
    "            \n",
    "\n",
    "    if CONFIG.use_aliked_lightglue:\n",
    "        model_name = \"aliked\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t,keypoints_count = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_aliked_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_aliked_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_aliked_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_aliked_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        total_keypoints+=keypoints_count\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_doghardnet_lightglue:\n",
    "        model_name = \"doghardnet\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t,keypoints_count = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_doghardnet_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_doghardnet_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_doghardnet_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_doghardnet_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        total_keypoints+=keypoints_count\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_superpoint_lightglue:\n",
    "        model_name = \"superpoint\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        \n",
    "        t,keypoints_count = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_superpoint_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_superpoint_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_superpoint_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_superpoint_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        total_keypoints+=keypoints_count\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)   \n",
    "        timings['feature_matching'].append(t)\n",
    "    #zhr: \n",
    "    if CONFIG.use_loftr:\n",
    "        file_keypoints = f\"{feature_dir}/matches_loftr.h5\"\n",
    "        params_loftr = {\n",
    "            \"min_matches\": 15,\n",
    "        }\n",
    "        detect_loftr(\n",
    "            img_fnames, index_pairs, feature_dir, device, params_loftr, file_keypoints\n",
    "        )\n",
    "        files_keypoints.append(file_keypoints)\n",
    "    # if CONFIG.use_sift:\n",
    "    #     file_keypoints = f\"{feature_dir}/matches_sift.h5\"\n",
    "    #     params_sift = {\n",
    "    #         \"min_matches\": 15,\n",
    "    #     }\n",
    "    #     detect_sift(\n",
    "    #         img_fnames, index_pairs, feature_dir, device, params_sift, file_keypoints\n",
    "    #     )\n",
    "    #     files_keypoints.append(file_keypoints)\n",
    "    # print(\"!!!!!!!!!!!\")\n",
    "    # print(f\"total keypoints:{total_keypoints}\")\n",
    "\n",
    "\n",
    "    #############################################################\n",
    "    # merge keypoints\n",
    "    #############################################################\n",
    "    keypoints_merger(\n",
    "        img_fnames,\n",
    "        index_pairs,\n",
    "        files_keypoints,\n",
    "        feature_dir = feature_dir,\n",
    "        filter_FundamentalMatrix = CONFIG.MERGE_PARAMS[\"filter_FundamentalMatrix\"],\n",
    "        filter_iterations = CONFIG.MERGE_PARAMS[\"filter_iterations\"],\n",
    "        filter_threshold = CONFIG.MERGE_PARAMS[\"filter_threshold\"],\n",
    "    )    \n",
    "    return timings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# zhr: ploting the 3D points through the colmap values.\n",
    "def plot_3d_reconstruction(maps, best_idx):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # 绘制相机位姿\n",
    "    for img_id, image in maps[best_idx].images.items():\n",
    "        t = image.tvec\n",
    "        ax.scatter(t[0], t[1], t[2], c='r', marker='o')  # 相机位置\n",
    "\n",
    "    # 如果重建结果有3D点，可以绘制它们\n",
    "    if maps[best_idx].points3D:\n",
    "        for point_id, point3D in maps[best_idx].points3D.items():\n",
    "            x, y, z = point3D.xyz\n",
    "            ax.scatter(x, y, z, c='b', marker='^')  # 3D点\n",
    "\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    plt.savefig(\"output_image.png\")\n",
    "    plt.show()\n",
    "\n",
    "def reconstruct_from_db(dataset, scene, feature_dir, img_dir, timings, image_paths):\n",
    "    scene_result = {}\n",
    "    #############################################################\n",
    "    # regist keypoints from h5 into colmap db\n",
    "    #############################################################\n",
    "    database_path = f'{feature_dir}/colmap.db'\n",
    "    if os.path.isfile(database_path):\n",
    "        os.remove(database_path)\n",
    "    gc.collect()\n",
    "    import_into_colmap(img_dir, feature_dir=feature_dir, database_path=database_path)\n",
    "    output_path = f'{feature_dir}/colmap_rec'\n",
    "\n",
    "    #############################################################\n",
    "    # Calculate fundamental matrix with colmap api\n",
    "    #############################################################\n",
    "    t=time()\n",
    "    options = pycolmap.SiftMatchingOptions()\n",
    "    #options.confidence = 0.9999\n",
    "    #options.max_num_trials = 20000\n",
    "    pycolmap.match_exhaustive(database_path, sift_options=options)\n",
    "    t=time() - t \n",
    "    timings['RANSAC'].append(t)\n",
    "    print(f'RANSAC in  {t:.4f} sec')\n",
    "\n",
    "    #############################################################\n",
    "    # Execute bundle adjustmnet with colmap api\n",
    "    # --> Bundle adjustment Calcs Camera matrix, R and t\n",
    "    #############################################################\n",
    "    t=time()\n",
    "    # By default colmap does not generate a reconstruction if less than 10 images are registered. Lower it to 3.\n",
    "    mapper_options = pycolmap.IncrementalMapperOptions()\n",
    "    #mapper_options.num_threads = 1\n",
    "    mapper_options.min_model_size = 3\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    maps = pycolmap.incremental_mapping(database_path=database_path, image_path=img_dir, output_path=output_path, options=mapper_options)\n",
    "    print(maps)\n",
    "    clear_output(wait=False)\n",
    "    t=time() - t\n",
    "    timings['Reconstruction'].append(t)\n",
    "    print(f'Reconstruction done in  {t:.4f} sec')\n",
    "    \n",
    "\n",
    "    #############################################################\n",
    "    # Extract R,t from maps \n",
    "    #############################################################            \n",
    "    imgs_registered  = 0\n",
    "    best_idx = None\n",
    "    list_num_images = []            \n",
    "    print (\"Looking for the best reconstruction\")\n",
    "    if isinstance(maps, dict):\n",
    "        for idx1, rec in maps.items():\n",
    "            print (idx1, rec.summary())\n",
    "            list_num_images.append( len(rec.images) )\n",
    "            if len(rec.images) > imgs_registered:\n",
    "                imgs_registered = len(rec.images)\n",
    "                best_idx = idx1\n",
    "    list_num_images = np.array(list_num_images)\n",
    "    print(f\"list_num_images = {list_num_images}\")\n",
    "    if best_idx is not None:\n",
    "        print (maps[best_idx].summary())\n",
    "        for k, im in maps[best_idx].images.items():\n",
    "            key1 = f'test/{dataset}/images/{im.name}'\n",
    "            scene_result[key1] = {}\n",
    "            # scene_result[key1][\"R\"] = deepcopy(im.cam_from_world.rotation.matrix())\n",
    "            # scene_result[key1][\"t\"] = deepcopy(np.array(im.cam_from_world.translation))\n",
    "            scene_result[key1][\"R\"] = deepcopy(im.rotmat())\n",
    "            scene_result[key1][\"t\"] = deepcopy(np.array(im.tvec))\n",
    "\n",
    "    print(f'Registered: {dataset} / {scene} -> {len(scene_result)} images')\n",
    "    print(f'Total: {dataset} / {scene} -> {len(image_paths)} images')\n",
    "    print(timings)\n",
    "    plot_3d_reconstruction(maps, best_idx) # zhr: Here is the output of reconstruction.\n",
    "    return scene_result\n",
    "\n",
    "def arr_to_str(a):\n",
    "    return ';'.join([str(x) for x in a.reshape(-1)])\n",
    "\n",
    "# Function to create a submission file.\n",
    "def create_submission(out_results, data_dict):\n",
    "    with open(f'submission.csv', 'w') as f:\n",
    "        f.write('image_path,dataset,scene,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in data_dict:\n",
    "            if dataset in out_results:\n",
    "                res = out_results[dataset]\n",
    "            else:\n",
    "                res = {}\n",
    "            for scene in data_dict[dataset]:\n",
    "                if scene in res:\n",
    "                    scene_res = res[scene]\n",
    "                else:\n",
    "                    scene_res = {\"R\":{}, \"t\":{}}\n",
    "                for image in data_dict[dataset][scene]:\n",
    "                    if image in scene_res:\n",
    "                        print (image)\n",
    "                        R = scene_res[image]['R'].reshape(-1)\n",
    "                        T = scene_res[image]['t'].reshape(-1)\n",
    "                    else:\n",
    "                        R = np.eye(3).reshape(-1)\n",
    "                        T = np.zeros((3))\n",
    "                    f.write(f'{image},{dataset},{scene},{arr_to_str(R)},{arr_to_str(T)}\\n')\n",
    "\n",
    "# import os\n",
    "# import gc\n",
    "# import concurrent.futures\n",
    "# from collections import defaultdict\n",
    "\n",
    "# # Configuration class or module (assuming CONFIG and values are defined somewhere)\n",
    "# class CONFIG:\n",
    "#     NUM_CORES = 4  # Example value\n",
    "#     use_superglue = True\n",
    "#     DRY_RUN = False  # Example value\n",
    "#     DRY_RUN_MAX_IMAGES = 10  # Example value\n",
    "\n",
    "# # Specify the directory containing the images.\n",
    "# src = '/home/zhr/user/3D-reconstruction/kaggle/input/image-matching-challenge-2024'\n",
    "# image_dir = f'{src}/test/church/images'\n",
    "\n",
    "# # Initialize the data dictionary.\n",
    "# data_dict = {}\n",
    "\n",
    "# # Helper function to ensure keys exist in the dictionary\n",
    "# def ensure_scene_key(data_dict, dataset, scene):\n",
    "#     if dataset not in data_dict:\n",
    "#         data_dict[dataset] = {}\n",
    "#     if scene not in data_dict[dataset]:\n",
    "#         data_dict[dataset][scene] = []\n",
    "\n",
    "# # Populate the data_dict with images from the specified directory.\n",
    "# for image_file in os.listdir(image_dir):\n",
    "#     if image_file.endswith(('.png', '.jpg', '.jpeg')):  # Adjust extensions based on your dataset\n",
    "#         dataset = \"church\"\n",
    "#         scene = \"image\"\n",
    "#         ensure_scene_key(data_dict, dataset, scene)\n",
    "#         data_dict[dataset][scene].append(image_file)\n",
    "\n",
    "#         # Print statements for debugging\n",
    "#         print(f\"Added {image_file} to dataset {dataset}, scene {scene}\")\n",
    "\n",
    "#         if CONFIG.DRY_RUN:\n",
    "#             if len(data_dict[dataset][scene]) == CONFIG.DRY_RUN_MAX_IMAGES:\n",
    "#                 break\n",
    "\n",
    "# print(data_dict)\n",
    "\n",
    "# # Print the structure of data_dict for verification\n",
    "# for dataset in data_dict:\n",
    "#     for scene in data_dict[dataset]:\n",
    "#         print(f'{dataset} / {scene} -> {len(data_dict[dataset][scene])} images')\n",
    "\n",
    "# out_results = {}\n",
    "# timings = {\n",
    "#     \"rotation_detection\": [],\n",
    "#     \"shortlisting\": [],\n",
    "#     \"feature_detection\": [],\n",
    "#     \"feature_matching\": [],\n",
    "#     \"RANSAC\": [],\n",
    "#     \"Reconstruction\": []\n",
    "# }\n",
    "# datasets = list(data_dict.keys())\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "# # Define a top-level function for processing images\n",
    "# def process_images(image_filenames, feature_dir):\n",
    "#     # Simulate some processing and return a dummy result\n",
    "#     print(f\"Processing images: {image_filenames} into {feature_dir}\")\n",
    "#     return {\"processed_images\": len(image_filenames)}\n",
    "\n",
    "# with concurrent.futures.ProcessPoolExecutor(max_workers=CONFIG.NUM_CORES) as executors:\n",
    "#     futures = defaultdict(dict)\n",
    "#     for dataset in datasets:\n",
    "#         print(f\"Processing dataset: {dataset}\")\n",
    "#         if dataset not in out_results:\n",
    "#             out_results[dataset] = {}\n",
    "#         for scene in data_dict[dataset]:\n",
    "#             print(f\"Processing scene: {scene}\")\n",
    "#             img_dir = image_dir\n",
    "#             print(f\"Image directory: {img_dir}\")\n",
    "#             if not os.path.exists(img_dir):\n",
    "#                 print(f\"Directory does not exist: {img_dir}\")\n",
    "#                 continue\n",
    "\n",
    "#             out_results[dataset][scene] = {}\n",
    "#             img_fnames = [f'{img_dir}/{x}' for x in data_dict[dataset][scene]]\n",
    "#             print(f\"Image filenames: {img_fnames}\")\n",
    "#             print(f\"Got {len(img_fnames)} images\")\n",
    "#             feature_dir = f'featureout/{dataset}_{scene}'\n",
    "#             if not os.path.isdir(feature_dir):\n",
    "#                 os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "#             # Submit the process_images function, not a lambda\n",
    "#             futures[dataset][scene] = executors.submit(process_images, img_fnames, feature_dir)\n",
    "#             print(f\"Submitted job for dataset {dataset}, scene {scene}\")\n",
    "\n",
    "#     # Ensure futures dict is populated correctly\n",
    "#     for dataset in futures:\n",
    "#         for scene in futures[dataset]:\n",
    "#             print(f\"{dataset} / {scene} -> {futures[dataset][scene]}\")\n",
    "\n",
    "# # Ensure empty 'scene' key checking before accessing the future result\n",
    "# for dataset in datasets:\n",
    "#     for scene in data_dict[dataset]:\n",
    "#         if scene not in futures[dataset]:\n",
    "#             print(f\"Warning: Scene '{scene}' does not exist in futures for dataset '{dataset}'\")\n",
    "#             continue\n",
    "# # Print the final results for verification\n",
    "\n",
    "# import os\n",
    "# import gc\n",
    "# import concurrent.futures\n",
    "# from collections import defaultdict\n",
    "\n",
    "# # Specify the directory containing the images.\n",
    "# src = '/home/zhr/user/3D-reconstruction/kaggle/input/image-matching-challenge-2024'\n",
    "# image_dir = f'{src}/test/church/images'\n",
    "\n",
    "# # Initialize the data dictionary.\n",
    "# data_dict = {}\n",
    "\n",
    "# # Populate the data_dict with images from the specified directory.\n",
    "# for image_file in os.listdir(image_dir):\n",
    "#     if image_file.endswith(('.png', '.jpg', '.jpeg')):  # Adjust extensions based on your dataset\n",
    "#         dataset = \"church\"\n",
    "#         scene = \"dataset\"\n",
    "#         if dataset not in data_dict:\n",
    "#             data_dict[dataset] = {}\n",
    "#         if scene not in data_dict[dataset]:\n",
    "#             data_dict[dataset][scene] = []\n",
    "#         data_dict[dataset][scene].append(image_file)\n",
    "#         print(image_file)\n",
    "#         if CONFIG.DRY_RUN:\n",
    "#             if len(data_dict[dataset][scene]) == CONFIG.DRY_RUN_MAX_IMAGES:\n",
    "#                 break\n",
    "\n",
    "# print(data_dict)\n",
    "# print(dataset)\n",
    "# print(\"#####\")                    \n",
    "# for dataset in data_dict:\n",
    "#     for scene in data_dict[dataset]:\n",
    "#         print(f'{dataset} / {scene} -> {len(data_dict[dataset][scene])} images')\n",
    "\n",
    "# out_results = {}\n",
    "# timings = {\n",
    "#     \"rotation_detection\" : [],\n",
    "#     \"shortlisting\":[],\n",
    "#     \"feature_detection\": [],\n",
    "#     \"feature_matching\":[],\n",
    "#     \"RANSAC\": [],\n",
    "#     \"Reconstruction\": []\n",
    "# }\n",
    "# datasets = list(data_dict.keys())\n",
    "# gc.collect()\n",
    "# datasets = []\n",
    "# for dataset in data_dict:\n",
    "#     datasets.append(dataset)\n",
    "\n",
    "# with concurrent.futures.ProcessPoolExecutor(max_workers=CONFIG.NUM_CORES) as executors:\n",
    "#     futures = defaultdict(dict)\n",
    "#     for dataset in datasets:\n",
    "#         print(dataset)\n",
    "#         if dataset not in out_results:\n",
    "#             out_results[dataset] = {}\n",
    "#         for scene in data_dict[dataset]:\n",
    "#             print(scene)\n",
    "#             img_dir = image_dir  # Use the specified image directory\n",
    "#             print(img_dir)\n",
    "#             if not os.path.exists(img_dir):\n",
    "#                 print(\"DONOTEXIST\")\n",
    "#                 continue\n",
    "\n",
    "#             out_results[dataset][scene] = {}\n",
    "#             img_fnames = [f'{img_dir}/{x}' for x in data_dict[dataset][scene]]\n",
    "#             print(img_fnames)\n",
    "#             print(f\"Got {len(img_fnames)} images\")\n",
    "#             feature_dir = f'featureout/{dataset}_{scene}'\n",
    "#             if not os.path.isdir(feature_dir):\n",
    "#                 os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# src = '/home/zhr/user/3D-reconstruction/kaggle/input/image-matching-challenge-2024'\n",
    "\n",
    "# # Get data from csv.\n",
    "# data_dict = {}\n",
    "# with open(f'{src}/sample_submission.csv', 'r') as f:\n",
    "#     for i, l in enumerate(f):\n",
    "#         # Skip header.\n",
    "#         if l and i > 0:\n",
    "#             image, dataset, scene, _, _ = l.strip().split(',')\n",
    "#             if dataset not in data_dict:\n",
    "#                 data_dict[dataset] = {}\n",
    "#             if scene not in data_dict[dataset]:\n",
    "#                 data_dict[dataset][scene] = []\n",
    "#             data_dict[dataset][scene].append(image)\n",
    "#             print(image)\n",
    "#             if CONFIG.DRY_RUN:\n",
    "#                 if len(data_dict[dataset][scene]) == CONFIG.DRY_RUN_MAX_IMAGES:\n",
    "#                     break\n",
    "# print(data_dict)\n",
    "# print(dataset)\n",
    "# print(\"#####\")                    \n",
    "# for dataset in data_dict:\n",
    "#     for scene in data_dict[dataset]:\n",
    "#         print(f'{dataset} / {scene} -> {len(data_dict[dataset][scene])} images')\n",
    "\n",
    "# out_results = {}\n",
    "# timings = {\n",
    "#     \"rotation_detection\" : [],\n",
    "#     \"shortlisting\":[],\n",
    "#    \"feature_detection\": [],\n",
    "#    \"feature_matching\":[],\n",
    "#    \"RANSAC\": [],\n",
    "#    \"Reconstruction\": []\n",
    "# }\n",
    "\n",
    "# gc.collect()\n",
    "# datasets = []\n",
    "# for dataset in data_dict:\n",
    "#     datasets.append(dataset)\n",
    "\n",
    "# with concurrent.futures.ProcessPoolExecutor(max_workers=CONFIG.NUM_CORES) as executors:\n",
    "#     futures = defaultdict(dict)\n",
    "#     for dataset in datasets:\n",
    "#         print(dataset)\n",
    "#         if dataset not in out_results:\n",
    "#             out_results[dataset] = {}\n",
    "#         for scene in data_dict[dataset]:\n",
    "#             print(scene)\n",
    "#             # Fail gently if the notebook has not been submitted and the test data is not populated.\n",
    "#             # You may want to run this on the training data in that case?\n",
    "#             img_dir = f'{src}/test/{dataset}/images'\n",
    "#             print(img_dir)\n",
    "#             if not os.path.exists(img_dir):\n",
    "#                 print(\"DONOTEXIST\")\n",
    "#                 continue\n",
    "\n",
    "#             out_results[dataset][scene] = {}\n",
    "#             img_fnames = [f'{src}/{x}' for x in data_dict[dataset][scene]]\n",
    "#             print(img_fnames)\n",
    "#             print (f\"Got {len(img_fnames)} images\")\n",
    "#             feature_dir = f'featureout/{dataset}_{scene}'\n",
    "#             if not os.path.isdir(feature_dir):\n",
    "#                 os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "            #############################################################\n",
    "            # get image rotations\n",
    "            #############################################################\n",
    "    #         t = time()\n",
    "    #         rots = [0 for _ in img_fnames]\n",
    "    #         t = time() - t\n",
    "    #         timings['rotation_detection'].append(t)\n",
    "    #         print(f'rotation_detection for {len(img_fnames)} images : {t:.4f} sec')\n",
    "    #         gc.collect()\n",
    "\n",
    "    #         #############################################################\n",
    "    #         # get image pairs\n",
    "    #         #############################################################\n",
    "    #         t = time()\n",
    "    #         index_pairs = get_image_pairs_shortlist_nn(img_fnames, nneighbor=6, exhaustive_if_less=20)\n",
    "    #         t = time() - t\n",
    "    #         timings['shortlisting'].append(t)\n",
    "    #         print(f'{len(index_pairs)}, pairs to match, {t:.4f} sec')\n",
    "    #         gc.collect()\n",
    "\n",
    "    #         #############################################################\n",
    "    #         # get keypoints\n",
    "    #         #############################################################\n",
    "    #         keypoints_timings = wrapper_keypoints(img_fnames, index_pairs, feature_dir, device, timings, rots)\n",
    "    #         timings['feature_matching'] = keypoints_timings['feature_matching']\n",
    "    #         gc.collect()\n",
    "\n",
    "    #         #############################################################\n",
    "    #         # kick COLMAP reconstruction\n",
    "    #         #############################################################\n",
    "    #         futures[dataset][scene] = executors.submit(\n",
    "    #             reconstruct_from_db,\n",
    "    #             dataset, scene, feature_dir, img_dir, timings, data_dict[dataset][scene]\n",
    "    #         )\n",
    "\n",
    "    # #############################################################\n",
    "    # # reconstruction results\n",
    "    # #############################################################\n",
    "    # for dataset in datasets:\n",
    "    #     for scene in data_dict[dataset]:\n",
    "    #         # wait to complete COLMAP reconstruction\n",
    "    #         result = futures[dataset][scene].result()\n",
    "    #         if result is not None:\n",
    "    #             out_results[dataset][scene] = result   # get R and t from result\n",
    "\n",
    "    # create_submission(out_results, data_dict)\n",
    "    # gc.collect()\n",
    "\n",
    "# Should Be Modified!!\n",
    "src = '/home/zhr/user/3D-reconstruction/kaggle/input/image-matching-challenge-2024'\n",
    "\n",
    "# zhr: Here is how we can modifies our data path instead of using kaggle_dataset.\n",
    "\n",
    "# import os\n",
    "# # Define your dataset and scene names (you can choose any names)\n",
    "# dataset = 'my_dataset'\n",
    "# scene = 'my_scene'\n",
    "# # Initialize data_dict\n",
    "# data_dict = {dataset: {scene: []}}\n",
    "# # Get all image file names from the specified directory\n",
    "# image_dir = src  # Assuming src is set to '/path/to/your/images'\n",
    "# image_extensions = ['.jpg', '.png', '.jpeg', '.bmp', '.tif', '.tiff']\n",
    "# # List all image files in the directory\n",
    "# for file_name in os.listdir(image_dir):\n",
    "#     if any(file_name.lower().endswith(ext) for ext in image_extensions):\n",
    "#         image_path = os.path.join(image_dir, file_name)\n",
    "#         # Add the image path relative to src\n",
    "#         relative_image_path = os.path.relpath(image_path, src)\n",
    "#         data_dict[dataset][scene].append(relative_image_path)\n",
    "\n",
    "# Get data from csv.\n",
    "data_dict = {}\n",
    "with open(f'{src}/sample_submission.csv', 'r') as f:\n",
    "    for i, l in enumerate(f):\n",
    "        # Skip header.\n",
    "        if l and i > 0:\n",
    "            image, dataset, scene, _, _ = l.strip().split(',')\n",
    "            if dataset not in data_dict:\n",
    "                data_dict[dataset] = {}\n",
    "            if scene not in data_dict[dataset]:\n",
    "                data_dict[dataset][scene] = []\n",
    "            data_dict[dataset][scene].append(image)\n",
    "            \n",
    "            if CONFIG.DRY_RUN:\n",
    "                if len(data_dict[dataset][scene]) == CONFIG.DRY_RUN_MAX_IMAGES:\n",
    "                    break\n",
    "                    \n",
    "for dataset in data_dict:\n",
    "    for scene in data_dict[dataset]:\n",
    "        print(f'{dataset} / {scene} -> {len(data_dict[dataset][scene])} images')\n",
    "\n",
    "out_results = {}\n",
    "timings = {\n",
    "    \"rotation_detection\" : [],\n",
    "    \"shortlisting\":[],\n",
    "   \"feature_detection\": [],\n",
    "   \"feature_matching\":[],\n",
    "   \"RANSAC\": [],\n",
    "   \"Reconstruction\": []\n",
    "}\n",
    "\n",
    "gc.collect()\n",
    "datasets = []\n",
    "for dataset in data_dict:\n",
    "    datasets.append(dataset)\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=CONFIG.NUM_CORES) as executors:\n",
    "    futures = defaultdict(dict)\n",
    "    for dataset in datasets:\n",
    "        print(dataset)\n",
    "        if dataset not in out_results:\n",
    "            out_results[dataset] = {}\n",
    "        for scene in data_dict[dataset]:\n",
    "            print(scene)\n",
    "            # Fail gently if the notebook has not been submitted and the test data is not populated.\n",
    "            # You may want to run this on the training data in that case?\n",
    "            img_dir = f'{src}/test/{dataset}/images'# zhr: Here may be modified.\n",
    "            if not os.path.exists(img_dir):\n",
    "                continue\n",
    "\n",
    "            out_results[dataset][scene] = {}\n",
    "            img_fnames = [f'{src}/{x}' for x in data_dict[dataset][scene]]# zhr: Be modified: img_fnames = [os.path.join(src, x) for x in data_dict[dataset][scene]] // Not tried yet.\n",
    "            print (f\"Got {len(img_fnames)} images\")\n",
    "            feature_dir = f'featureout/{dataset}_{scene}'\n",
    "            if not os.path.isdir(feature_dir):\n",
    "                os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "            #############################################################\n",
    "            # get image rotations\n",
    "            #############################################################\n",
    "            t = time()\n",
    "            rots = [ 0 for fname in img_fnames ]\n",
    "            t = time()-t\n",
    "            timings['rotation_detection'].append(t)\n",
    "            print (f'rotation_detection for {len(img_fnames)} images : {t:.4f} sec')\n",
    "            gc.collect()\n",
    "            \n",
    "            #############################################################\n",
    "            # get image pairs\n",
    "            #############################################################\n",
    "            t=time()\n",
    "\n",
    "            index_pairs = get_image_pairs_shortlist_nn(img_fnames,\n",
    "                                                nneighbor=42,              \n",
    "                                                exhaustive_if_less = 42,)\n",
    "            t=time() -t \n",
    "            timings['shortlisting'].append(t)\n",
    "            print (f'{len(index_pairs)}, pairs to match, {t:.4f} sec')\n",
    "            gc.collect()\n",
    "\n",
    "            #############################################################\n",
    "            # get keypoints\n",
    "            #############################################################            \n",
    "            keypoints_timings = wrapper_keypoints(\n",
    "                img_fnames, index_pairs, feature_dir, device, timings, rots\n",
    "            )\n",
    "            timings['feature_matching'] = keypoints_timings['feature_matching']\n",
    "            gc.collect()\n",
    "\n",
    "            #############################################################\n",
    "            # kick COLMAP reconstruction\n",
    "            #############################################################            \n",
    "            futures[dataset][scene] = executors.submit(\n",
    "                reconstruct_from_db, \n",
    "                dataset, scene, feature_dir, img_dir, timings, data_dict[dataset][scene])\n",
    "                \n",
    "    #############################################################\n",
    "    # reconstruction results\n",
    "    #############################################################            \n",
    "    for dataset in datasets:\n",
    "        for scene in data_dict[dataset]:\n",
    "            # wait to complete COLMAP reconstruction\n",
    "            result = futures[dataset][scene].result()\n",
    "            if result is not None:\n",
    "                out_results[dataset][scene] = result   # get R and t from result\n",
    "    \n",
    "    create_submission(out_results, data_dict)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "731d7bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoFTR is installed and ready to use.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"LoFTR is installed and ready to use.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8143495,
     "sourceId": 71885,
     "sourceType": "competition"
    },
    {
     "datasetId": 2058261,
     "sourceId": 3414836,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3117886,
     "sourceId": 5373920,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3364321,
     "sourceId": 5850511,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4628051,
     "sourceId": 7884485,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 170475544,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 170565695,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 174129945,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 175679956,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 175684111,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 176463227,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 2663,
     "sourceId": 3736,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 2662,
     "sourceId": 3735,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 860.578212,
   "end_time": "2024-06-11T09:37:15.575892",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-11T09:22:54.997680",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
